{"cells":[{"cell_type":"markdown","source":["# Team 7 Airline Delays\n\n## Additional Materials\nGithub Repo:\nhttps://github.com/UCB-w261/su20-project-su20-team7\n\nEDA Notebook:\nhttps://dbc-9bfca75b-725c.cloud.databricks.com/?o=52497824689954#notebook/2542098129693526/command/2542098129693527"],"metadata":{}},{"cell_type":"markdown","source":["##1 Introduction to the Problem\nThe FAA annually handles over 16 million flights out of 20,000 airports (about 5K public commercial and 15K private) to service 1 billion passengers and 44 trillion tons of freight. This translates to about 44,000 flights operating daily serving about 3 million passengers. The aviation industry supports 10 million jobs and contributes 5.1% to the U.S. GDP. Given the expanse and complexity of operations, delays in airline and airport operations are therefore inevitable. As of 2018, almost 1 in 5 US flights suffered a delay and as of 2019, worldwide costs due to airline delays were estimated at $60 billion, with over a half incurred due to delays in US domestic as well as international flights. Economic costs of delayed flights not only include the direct effects of increased airline costs (due to crew, fuel, aircraft and maintenance) and airport costs (operations, maintenance, supporting service provides concessions, transportation) but also the indirect effects of lost labour productivity for business travellers, an opportunity cost of time for leisure travellers (due to schedule buffer, delayed flights, flight cancellations and missed connections), and changes in consumer spending on travel and tourism goods and services. As of 2019 US aviation reported a fuel wastage of 740 million gallons and CO2  emissions amounting to 7.1 million tons. Clearly this problem has a huge impact in several areas. \n\nWith the FAA forecasting an increase of 65% over the next couple decades in revenue passenger miles, finding solutions to mitigate delays is imperative. A 2012 study suggests that “even for modest reductions in flight delay, the economic benefits are substantial. US net welfare would increase by $17.6 billion for a 10 per cent reduction in flight delay and by $38.5 billion for a 30 per cent reduction. ”"],"metadata":{}},{"cell_type":"markdown","source":["[Slide01-Rad]"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays_overtime.png?token=AFP5XG375TQCOWM6FCPEHS27GVNEQ' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays_overtime.png?token=AFP5XG375TQCOWM6FCPEHS27GVNEQ' >"]}}],"execution_count":4},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/flights%20by%20year.png?token=AKFL6ADLX5OBKYO2FDHF4NK7GVP42' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/flights%20by%20year.png?token=AKFL6ADLX5OBKYO2FDHF4NK7GVP42' >"]}}],"execution_count":5},{"cell_type":"markdown","source":["Efforts so far to address this problem have taken the shape of expanding aviation infrastructure, improvements to existing air traffic management systems and congestion pricing. The first is costly and depends upon availability of real-estate and funds. OHare added a 5th runway recently and is investing 8.5 billion for terminal and runway expansions, 10 additional gates, 2 satellite terminals, and 60,000 jobs.  The second improves airport capacity by better air traffic management, while the third works on reducing demand during peak periods and its effectiveness is based on the willingness of travellers to consider off-peak pricing. \n\nComputationally speaking, researchers have taken several approaches to tackle this problem. Many involve statistical analysis, probabilistic modeling, network modeling,  and operations research techniques with most solutions focused on reducing aggregate delays at airports by forecasting the same or trying to understand the propagation of delays after the first delay has already occurred. \n\nWhile this problem has been computationally intensive and complex to solve historically, in recent years, improvements in big data technology and machine learning have led to new research that involves developing scalable algorithms that can predict delays at an individual flight level with the help of openly sourced data. While no such solutions are currently “operationalized”, these are imagined to work alongside existing Air Traffic Management Systems to improve air space management, and help airlines, airports and passengers respond to delays and mitigate losses proactively.\n\nOur project showcases the design of one such approach. Flight Performance Data from the FAA (Federal Aviation Authority) and meteorological information from NOAA (National Oceanic and Atmospheric Administration) are considered to make a prediction of whether a flight would incur a delay of greater than 15 minutes or not. This process is expected to be run 2 hrs prior to flight departures at any point of time (giving airlines and airports time to regroup and passengers proactively upon a delay), and deliver a prediction accuracy of about 85% (which is the current industry best)."],"metadata":{}},{"cell_type":"markdown","source":["[Slide2-Rad]"],"metadata":{}},{"cell_type":"markdown","source":["#### Goal: To develop an algorithm that would predict whether a flight is going to be delayed by >15 min, given airlines and weather data, 2hrs prior to flight departures, with a target weighted precision score of at least 85% and a target prediction time of 30 minutes."],"metadata":{}},{"cell_type":"markdown","source":["###Open Datasets\n\nThe passenger flights on-time performance data primarily consisted of origin-destination, carrier, departure/arrival performance, delays, causes of delay,  flight summaries, gate info and diverted flight information (a total of 109 fields ). \n\nThe weather data consisted of primary weather measurements such as temperature, precipitation, wind, humidity etc. taken hourly at various weather stations world over, along with several other data related to the quality and frequency of the primary measurements (a total of 177 fields). The existence of these data in parquet format in the Databricks file system enabled pushdown querying and efficient joins.\n\nThe airports data and the holidays list were scraped from the web."],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/data%20specifications.png?token=AKFL6AHQKKUWMLIW7YQELBK7GVRBC' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/data%20specifications.png?token=AKFL6AHQKKUWMLIW7YQELBK7GVRBC' >"]}}],"execution_count":10},{"cell_type":"markdown","source":["### Initial EDA\nAs a first step, we attempted to understand a few aspects of this data. \n\nFirst, the distribution of delays by various characteristics and second, distribution of delays by delay type, and more specifically, the contribution of weather to these delays. \n\nThe former would help us understand what elements of the airlines’ data could potentially be used to signal a delay, while the latter would help us determine if there are any particular weather related fields that could help us improve the accuracy of our forecasts more so than others."],"metadata":{}},{"cell_type":"markdown","source":["[Slide3-Rad]"],"metadata":{}},{"cell_type":"markdown","source":["####Delay Analysis\n - by delay duration\n - by type of delay\n - weather's contribution"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays%20by%20duration.png?token=AKFL6AGY3VB6GMRQIG2DWK27GVSXK' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays%20by%20duration.png?token=AKFL6AGY3VB6GMRQIG2DWK27GVSXK' >"]}}],"execution_count":14},{"cell_type":"markdown","source":["#### Delay Types"],"metadata":{}},{"cell_type":"markdown","source":["[Slide4-Rad]"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays%20by%20delay%20type.png?token=AKFL6AB2HFX6FFB2KVITTF27GVR7E' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays%20by%20delay%20type.png?token=AKFL6AB2HFX6FFB2KVITTF27GVR7E' >"]}}],"execution_count":17},{"cell_type":"markdown","source":["####Weather's Contribution\nOur initial exploratory analysis and further research around delays revealed that while flight delays vary by carrier, day of week , month of year and distance, the variation of delays attributed due to weather were not completely captured by the WEATHER_DELAY field as we first imagined. \n\nIt was useful to learn that the  WEATHER_DELAY field only captures about 6% of the total weather related delays while the remaining are actually included in the NAS_DELAY  and LATE_AIRCRAFT_DELAY. \n\nPrior research in the area and information by BTS (Bureau of Transportation Statistics) confirmed that weather delays may constitute as much as 40% of the total delays and as much as 20% of delays are caused by Late Arriving aircrafts.\n\nThis knowledge helped us establish perspective on a few things:\n - to avoid the potential pitfall of eliminating records that were in some instances not coded as delayed due to weather for training\n - to place undue importance on weather measurements as being the dominating  predictor of delay. \n - to experiment and compare the accuracy of different models, one trained on features that include weather related data Vs another trained on features that include just flight information.\n - while weather data is important, since joins between massive tables could be expensive, we could consider bringing in a subset of columns required for the success of our model without negatively impacting the performance of our data pipeline."],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/weather%20share%20of%20delays.png?token=AKFL6ADY3HXQMKW5JDLR4EC7GVTP6' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/weather%20share%20of%20delays.png?token=AKFL6ADY3HXQMKW5JDLR4EC7GVTP6' >"]}}],"execution_count":19},{"cell_type":"markdown","source":["[Slide9-Sirak]"],"metadata":{}},{"cell_type":"markdown","source":["### Data Distributions\n\nImages from our EDA Notebook, linked: https://dbc-9bfca75b-725c.cloud.databricks.com/?o=52497824689954#notebook/2542098129693526/command/2542098129693527"],"metadata":{}},{"cell_type":"markdown","source":["#### Delays by Airport"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/AirportTallies.png?token=AFP5XG347UKOP62XQUNTCZS7GWBUK' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/AirportTallies.png?token=AFP5XG347UKOP62XQUNTCZS7GWBUK' >"]}}],"execution_count":23},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays_USmap.png?token=AFP5XGZG72HRBYTCABVWA6C7GWBVG' width=800 height=550>\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays_USmap.png?token=AFP5XGZG72HRBYTCABVWA6C7GWBVG' width=800 height=550>"]}}],"execution_count":24},{"cell_type":"markdown","source":["#### Delays over Time"],"metadata":{}},{"cell_type":"code","source":["# Hypothesis, long holiday's are a good indicator for delays.\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/long_weekends.png?token=AFP5XG2V4LPKZAL36LLHC6K7GWBV2' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/long_weekends.png?token=AFP5XG2V4LPKZAL36LLHC6K7GWBV2' >"]}}],"execution_count":26},{"cell_type":"code","source":["# Tip! The last Thurs of Nov is best day of the year to avoid delays\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays_overtime.png?token=AFP5XGYPI2ROEHSNU5YPBJK7GWBVO' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/delays_overtime.png?token=AFP5XGYPI2ROEHSNU5YPBJK7GWBVO' >"]}}],"execution_count":27},{"cell_type":"markdown","source":["#### Weather Distributions\n\nThe following are taken from a 1% sample of the training years 2015-2017"],"metadata":{}},{"cell_type":"code","source":["# We decided to keep values entered as \"Missing\" due to their sheer number, \n# and added a one-hot encoding to keep track of these missing entries\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/temperatures.png?token=AFP5XG2Q6UPUBDZARLLLRPS7GWBXE' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/temperatures.png?token=AFP5XG2Q6UPUBDZARLLLRPS7GWBXE' >"]}}],"execution_count":29},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/dewpoints.png?token=AFP5XGZZ3H3DHELZIO5MUC27GWBVU' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/dewpoints.png?token=AFP5XGZZ3H3DHELZIO5MUC27GWBVU' >"]}}],"execution_count":30},{"cell_type":"code","source":["# Wind angles reporte as missing where windspeed was reported were imputed to zero as well, and not recorded as missing\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/windspeeds.png?token=AFP5XG275XE2SYFFCITSYBS7GWBXS' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/windspeeds.png?token=AFP5XG275XE2SYFFCITSYBS7GWBXS' >"]}}],"execution_count":31},{"cell_type":"code","source":["# Scatter plot with Airports columns\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/scatter_depdel_airlines.png?token=AFP5XG7WTRFBPN2TPOERA3S7GWEYI'  width=800 height=800>\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/scatter_depdel_airlines.png?token=AFP5XG7WTRFBPN2TPOERA3S7GWEYI'  width=800 height=800>"]}}],"execution_count":32},{"cell_type":"code","source":["# Scatter with weather columns\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/scatter_depdel_weather.png?token=AFP5XGZFUMDF7ZOBDM3W3B27GWEYA'  width=900 height=900>\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/scatter_depdel_weather.png?token=AFP5XGZFUMDF7ZOBDM3W3B27GWEYA'  width=900 height=900>"]}}],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["## Imports"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, TimestampType\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import trim\nimport plotly.express as px\nimport urllib.request\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import hour\nfrom pyspark.sql.functions import udf, concat, lit, col\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import TrainValidationSplit\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nimport pyspark.sql.functions as F\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.sql.types import FloatType\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n\nsqlContext = SQLContext(sc)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/context.py:77: DeprecationWarning:\n\nDeprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n\n</div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["## Utilities"],"metadata":{}},{"cell_type":"code","source":["# FUNCTION TO CHECK IF FILE EXISTS\n# From https://forums.databricks.com/questions/20129/how-to-check-file-exists-in-databricks.html\ndef file_exists(path):\n  try:\n    dbutils.fs.ls(path)\n    return True\n  except Exception as e:\n    if 'java.io.FileNotFoundException' in str(e):\n      return False\n    else:\n      raise\n      \n# FUNCTION TO COUNT NULLS/NANS\n# MAX: added some type safety\ndef check_nulls_nans(df):\n    null_counts_df = df.select([f.count(f.when(f.isnan(c.name) | f.isnull(c.name), c.name)).alias(c.name) for c in df.schema.fields if not isinstance(c.dataType, TimestampType) ])\n    return null_counts_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["## Environment"],"metadata":{}},{"cell_type":"code","source":["# ENVIRONMENTAL VARIABLES SETUP\n# Run validations: True | False\nVALIDATE=False\n# Verbose: True | False\nVERBOSE=False\n# Don't use persisted data on disk, instead recompute everything: True | False\nFORCE_COMPUTE_ALL=False\n# Persist everything to disk: True | False\nPERSIST_ALL=False\n# Persist models to disk: True | False\nPERSIST_MODELS = False\n# Datasets: \n# 'Toy': Initial dev data\n# '2015': All of 2015\n# '2015_2017': 2015-2017\nDATASETS='2015_2019'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["## User Environment"],"metadata":{}},{"cell_type":"code","source":["# Enter \"MR7\" to load models from disk\nuser_initials = \"MR7\"\nif user_initials == \"MR7\":\n  PERSIST_MODELS = False # ensuring that the golden models stored under MR7 folder are not overwritten!!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["[Slide5-Max]"],"metadata":{}},{"cell_type":"markdown","source":["### Data Pipeline \nFor the purpose of organizing our data engineering, we adopted a model like the one Databricks documents, using a metaphor of bronze, silver and gold datasets. We use the terms in this way:\n\n**Bronze** - thin facade over the raw data sources to make them usable in spark.\n\n**Silver** - processed, regularized, joined datasets\n\n**Gold** finished data ready for ML modeling\n\nThis helps divide tasks and responsibilities. EDA and data regularization (normalization, correction, imputation) happen in Silver. Feature engineering also happens in Silver, although high-level feature engineering such as feature selection and PCA happen in Gold. We found it useful to introduce sub-groups to Silver: \"clean\" tables have data regularization. Pure silver tables may be the result of joins and may be enriched with derived features."],"metadata":{}},{"cell_type":"markdown","source":["## Bronze Datasets \nBronze datasets will be named as follows:\n* airports_raw\n* weather_raw\n* airlines_raw\n* holidays_raw\n\nIn this first section, we will establish RDDs and SQL tables with those names. \n\nThere is no need to persist the bronze tables since they are just views of raw data."],"metadata":{}},{"cell_type":"markdown","source":["### airports_raw\n\nSource: https://openflights.org/data.html"],"metadata":{}},{"cell_type":"code","source":["# FUNCTION TO SCRAPE AIRPORTS DATA\ndef init_airports_raw():\n  # Note: the airports.dat data is very small, so we will always load it in full\n  source_url = 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat'\n  data_file_name = 'dbfs:/team7/data/airports.dat'\n  if not file_exists(data_file_name):\n    print(f\"Importing the data from {source_url}\")\n    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\", \"/tmp/airports.dat\") \n    dbutils.fs.mv(\"file:/tmp/airports.dat\", data_file_name)\n  else:\n    print(f\"Skipping import: data already exists at {data_file_name}\")\n  # display(dbutils.fs.ls(\"/data/airports.dat\"))\n  airports_schema = StructType([\n    StructField(\"Airport_ID\", IntegerType(), True),\n    StructField(\"Name\", StringType(), True),\n    StructField(\"City\", StringType(), True),\n    StructField(\"Country\", StringType(), True),\n    StructField(\"IATA\", StringType(), True),\n    StructField(\"ICAO\", StringType(), True),\n    StructField(\"Latitude\", DoubleType(), True),\n    StructField(\"Longitude\", DoubleType(), True),\n    StructField(\"Altitude\", IntegerType(), True),\n    StructField(\"Timezone\", DoubleType(), True),\n    StructField(\"DST\", StringType(), True),\n    StructField(\"Tz\", StringType(), True),\n    StructField(\"Type\", StringType(), True),\n    StructField(\"Source\", StringType(), True)\n  ])\n  df = spark.read.format(\"csv\") \\\n    .option(\"inferSchema\", \"false\") \\\n    .option(\"header\", \"false\") \\\n    .option(\"sep\", \",\") \\\n    .schema(airports_schema) \\\n    .load(data_file_name)\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":47},{"cell_type":"code","source":["# IMPORT AIRPORTS DATA & CREATE A VIEW\nairports_raw = init_airports_raw()\nairports_raw.createOrReplaceTempView(\"airports_raw\")\nprint(f\"airports_raw: {airports_raw.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Skipping import: data already exists at dbfs:/team7/data/airports.dat\nairports_raw: 7698 rows\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["# PRINT SCHEMA\nif VERBOSE:\n  airports_raw.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":49},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(airports_raw)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"markdown","source":["###Long Holidays \n(Via https://docs.google.com/spreadsheets/d/1zv-Sydffde6luh_lVTcFidjpjsK7ojjXRciAUelbQr4/edit#gid=749082549)"],"metadata":{}},{"cell_type":"code","source":["import requests\nfrom pyspark.sql.functions import lit\n\n# FUNCTION TO SCRAPE Long-Holidays DATA\ndef init_holidays_raw():\n    # Grab the Google Sheets doc and save it to file\n    source_url = 'https://docs.google.com/spreadsheet/ccc?key=1zv-Sydffde6luh_lVTcFidjpjsK7ojjXRciAUelbQr4&output=csv'\n    data_file_name = 'dbfs:/team7/data/holidays.csv'\n    if not file_exists(data_file_name):\n        print(f\"Importing the data from {source_url}\")\n        response = requests.get(source_url) \n        assert response.status_code == 200, 'Wrong status code'\n        dbutils.fs.put(data_file_name, str(response.content, 'utf-8'), True)\n    else:\n        print(f\"Skipping import: data already exists at {data_file_name}\")\n        \n    holidays_schema = StructType([\n        StructField(\"Year\", IntegerType(), True),\n        StructField(\"Date\", StringType(), True),\n        StructField(\"Weekday\", StringType(), True),\n        StructField(\"Name\", StringType(), False),\n        StructField(\"Type\", StringType(), False),\n        ])\n    \n    df = spark.read.format(\"csv\") \\\n        .option(\"inferSchema\", \"false\") \\\n        .option(\"header\", \"true\") \\\n        .option(\"sep\", \",\") \\\n        .schema(holidays_schema) \\\n        .load(data_file_name)\n    \n    date_split = f.split(df.Date, '/')\n\n    return (df\n            .withColumn(\"holiday\", lit(1))\n           )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":52},{"cell_type":"code","source":["# SCRAPE Long Holiday DATA\nholidays_raw = init_holidays_raw()\nholidays_raw.createOrReplaceTempView(\"holidays_raw\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Skipping import: data already exists at dbfs:/team7/data/holidays.csv\n</div>"]}}],"execution_count":53},{"cell_type":"code","source":["if VALIDATE:\n  display(holidays_raw)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":54},{"cell_type":"markdown","source":["### weather_raw\nSource: https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.ncdc:C00532"],"metadata":{}},{"cell_type":"code","source":["# FUNCTION TO IMPORT WEATHER DATA\ndef init_weather_raw():\n  print(\"Loading weather\")\n  if DATASETS == 'Toy':\n    df = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/weather_data/weather2015a.parquet\")\n\n    # extract weather data for the 1st quarter of 2015\n    df = df.filter(f.col('date').between(\"2015-01-01\", \"2015-04-01\"))\n  elif DATASETS == '2015':\n    df = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/weather_data/weather2015a.parquet\")\n  elif DATASETS == '2015_2017':\n    df = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/weather_data/weather201[5-7]a.parquet\")\n  elif DATASETS == '2015_2019':\n    df = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/weather_data/weather201[5-9]a.parquet\")\n  else:\n    raise RuntimeException(f\"Bad value for {DATASETS}\")\n  return df\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":56},{"cell_type":"code","source":["# IMPORT WEATHER DATA & CREATE A VIEW\nweather_raw = init_weather_raw()\nweather_raw.createOrReplaceTempView(\"weather_raw\")\nprint(f\"weather_raw: {weather_raw.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading weather\nweather_raw: 626994336 rows\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["# PRINT SCHEMA\nif VERBOSE:\n  weather_raw.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":58},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(weather_raw)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":59},{"cell_type":"code","source":["# verify quarterly counts\nquery = f\"\"\"\n    WITH quarters AS (\n    select date_trunc(\"QUARTER\", DATE) as quarter, \n           count(*) as count\n    from weather_raw\n    group by quarter\n    )\n    select date_format(quarter, \"yyyy\") as Year,\n           date_format(quarter, \"q\") as Quarter, \n           count from quarters\n    order by Year, Quarter\n\"\"\"\nif VERBOSE:\n  df = spark.sql(query)\n  display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"markdown","source":["### airlines_raw\nSource:"],"metadata":{}},{"cell_type":"code","source":["# FUNCTION TO IMPORT AIRLINES DATA\ndef init_airlines_raw():\n  if DATASETS == 'Toy':\n    airlines_raw = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data_3m/*.parquet\")\n  elif DATASETS == '2015':\n    airlines_raw = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data/2015.parquet\")\n  elif DATASETS == '2015_2017':\n    airlines_raw = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data/201[5-7].parquet\")\n  elif DATASETS == '2015_2019':\n    airlines_raw = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data/201[5-9].parquet\")\n  else:\n    raise RuntimeException(f\"Bad value for {DATASETS}\")\n  return airlines_raw\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":62},{"cell_type":"code","source":["# IMPORT AIRLINES DATA & CREATE A VIEW\nairlines_raw = init_airlines_raw()\nairlines_raw.createOrReplaceTempView(\"airlines_raw\")\nif VALIDATE:\n  print(f\"airlines_raw: {airlines_raw.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":63},{"cell_type":"code","source":["# PRINT SCHEMA\nif VERBOSE:\n  airlines_raw.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":64},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(airlines_raw)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":65},{"cell_type":"code","source":["# validate quarterly counts\nquery = f\"\"\"\n    WITH quarters AS (\n      select YEAR, QUARTER, count(*) as count\n      from airlines_raw\n      group by YEAR, QUARTER\n    )\n    select YEAR as Year,\n           QUARTER as Quarter, \n           count from quarters\n    order by Year, Quarter\n\"\"\"\nif VALIDATE:\n  print(query)\n  df = spark.sql(query)\n  display(df)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":66},{"cell_type":"code","source":["# join with airlines and holidays\ndef join_longHolidays_toAirlines(airlinesDF, holidaysDF):\n    df = (holidaysDF.select(\"holiday\",\"Date\")\n                  .withColumnRenamed(\"Date\",\"FL_Date\")\n          .join(airlinesDF, on=\"FL_Date\", how=\"right\") )\n    \n    return df.fillna({'holiday':0})\n  \nairlines_raw = join_longHolidays_toAirlines(airlines_raw,holidays_raw)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":67},{"cell_type":"code","source":["if VERBOSE :\n  display(airlines_raw)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":68},{"cell_type":"markdown","source":["## Silver Datasets\nThese have derived, cleaned and regularized data.\n* airports_clean\n* airlines_clean\n* airlines_silver (airlines joined with airports)\n* weather_clean\n* weather_silver (weather joined with airports)\n* silver_df (airlines-airports-weather)"],"metadata":{}},{"cell_type":"markdown","source":["####Data Cleaning\n\n[NS5]Once we figured these critical aspects we progressed with data import, cleansing, transforming, and joining our airlines and weather data.\n\nAs a part of data import, we imported the following raw data and stored it as our Bronze Data sets:\nAirports.data\nHolidays data\nAirlines data\nWeather data\n\nAs a part of data cleansing, we:\neliminated duplicates\nimputed nulls to zeroes wherever applicable\nhandled invalid time zones by setting them to Eastern\nAssessed the impact of invalid airport codes and airport codes that could not be mapped to weather stations. There were two flight origins (local/municipal airports) in our training data that could not be mapped to weather stations. These accounted to <1% of total flights. Given the characteristics, we decided to drop these from our training set downstream.\n\nAs a part of data transformations, we:\nconverted the arrival and departure times of flights to UTC\ngenerated hourly time buckets\nparsed compound weather data fields"],"metadata":{}},{"cell_type":"markdown","source":["### airports_clean\n - eliminate duplicates\n - select relevant columns\n - evaluate airport codes and time zones for completeness"],"metadata":{}},{"cell_type":"code","source":["# check for duplicates\nif VALIDATE:\n  unique_id = [\"IATA\", \"ICAO\", \"Tz\"]\n  w = Window.partitionBy(unique_id)\n  display(airports_raw.select('*', f.count(\"*\")\\\n                      .over(w).alias('dupCount'))\\\n                      .where('dupCount > 1')\\\n                      .drop('dupCount'))\n# OK => no duplicates "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":72},{"cell_type":"code","source":["# DROP DUPLICATES\nairports_tmp = airports_raw.dropDuplicates(subset = [\"IATA\", \"ICAO\", \"Tz\"])\nairports_tmp.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: 7698</div>"]}}],"execution_count":73},{"cell_type":"code","source":["# FILTER & SELECT RELEVANT COLUMNS\nairports_tmp = airports_tmp.select(\"IATA\", \"ICAO\", \"Name\", \"Tz\")\nairports_tmp.createOrReplaceTempView(\"airports_tmp\")\nairports_tmp.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: 7698</div>"]}}],"execution_count":74},{"cell_type":"code","source":["# check for nulls/missing values\nif VALIDATE:\n  df = check_nulls_nans(airports_tmp)\n  display(df)\n  \n# there appears to be 1 IATA with null/NaN"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":75},{"cell_type":"code","source":["# Investigating IATA = null/Nan...\nif VALIDATE:\n  display(airports_raw.filter(f.isnan('IATA') | f.isnull('IATA')))\n\n# This is not a US aiport. Since we are not going to have flights out of this airport, we can leave as is.\n# What's interesting is that 'NAN' is not a missing value NaN - it is infact a legit IATA code! Would be interesting to handle if we are predicting delays for flights out of Fiji :-)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":76},{"cell_type":"code","source":["# check for outliers\nif VALIDATE:\n  display(airports_tmp.describe())\n  \n# there appear to be some \\Ns in IATA, ICAO and Tz fields which are invalid values"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":77},{"cell_type":"code","source":["# Investigating Tz = '\\N'...\nif VALIDATE:\n  tmp = airports_tmp.select(\"*\").where(\"length(Tz) = 2\")\n#   display(tmp)\n  print(f\"airports with invalid Tzs: {tmp.count()}\")\n  \n# there seem to be 1021 airports with Tz = '\\N'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":78},{"cell_type":"code","source":["# Investigating the impact of Tz = '\\N'...\n# Do we have any airport origins that would map to IATAs with Tz = '\\N'? \nif VALIDATE:\n  airports_with_invalid_Tz = airports_tmp.select(\"*\").where(\"length(Tz) = 2\")\n  flight_origins = airlines_raw.select(\"ORIGIN\").distinct()\n  origins_with_invalid_Tzs = (flight_origins.withColumnRenamed(\"ORIGIN\", \"IATA\")\n                                 .join(airports_with_invalid_Tz, how=\"inner\", on=\"IATA\")\n                           )\n  print(f\"airports with Tz='\\\\N': {airports_with_invalid_Tz.count()}\")\n  print(f\"distinct airport origins: {flight_origins.count()}\")\n  print(f\"airport origins with invalid Tz: {origins_with_invalid_Tzs.count()}\")\n  \n  # there seem to be no airports that we have in the 3 year data that have Tz = '\\N' . Therefore no impact."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":79},{"cell_type":"code","source":["# Investigating the impact of Tz = '\\N'...\nif VALIDATE:\n  display(airports_tmp.select(\"*\").where(\"length(Tz) = 2 and Country = 'United States'\"))\n  \n# All such seem to be local/municipal airports\n# ASSUMPTION: we will not be asked to predict out of such airports"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":80},{"cell_type":"code","source":["# HANDLE Tz = '\\N'\n# In the event Tz = '\\N' or null, let's default to eastern time\nairports_tmp = ( airports_tmp.withColumn(\"Tz2\", f.when(airports_tmp.Tz.isNull() | (airports_tmp.Tz == '\\\\N'), \"America/New_York\").otherwise(airports_tmp.Tz))\n                      .drop('Tz')\n                      .withColumnRenamed(\"Tz2\", \"Tz\")\n                     )\nif VALIDATE:\n  null_Tz_df = airports_tmp.filter(f.isnan('Tz') | f.isnull('Tz'))\n  invalid_Tz_df = airports_tmp.select(\"*\").where(\"length(Tz) = 2\")\n  print(f\"Null Tzs: {null_Tz_df.count()}\")\n  print(f\"Invalid Tzs: {invalid_Tz_df.count()}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":81},{"cell_type":"code","source":["#Investigating IATAs = '\\N'...\nif VALIDATE:\n  tmp = airports_tmp.select(\"*\").where(\"length(IATA) != 3\")\n  print(f\"airports with invalid IATA: {tmp.count()}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":82},{"cell_type":"code","source":["#Investigating IATAs = '\\N'...\nif VALIDATE:\n  tmp = airports_tmp.select(\"*\").where(\"length(IATA) != 3\")\n  display(tmp)\n  \n# these seem to all be municipal/private/international airports with no official IATA designation from the FAA.\n# there is no way to \"create\" any designations for these\n# investigate further to see if there are any flights from origin airports such as these in the airlines data"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":83},{"cell_type":"code","source":["# Investigating IATA = '\\N' further...\n# Checking to see if there are any origin airports that cannot be mapped to IATA's in airports.dat that fall under a similar category as above\nif VALIDATE:\n  origin_airports = airlines_raw.select(\"ORIGIN\").distinct()\n  airports_IATAs = airports_tmp.select(\"IATA\").distinct()\n  join_df = origin_airports.withColumnRenamed(\"ORIGIN\", \"IATA\").join(airports_IATAs, how=\"left\", on=\"IATA\")\n  display(join_df.select(\"IATA\").where(\"length(IATA) != 3\"))\n  \n\n# there appear to be no flights out of airports with IATA='\\N', therefore no need to handle.\n# ASSUMPTION: we will not be asked to predict delays for such airports\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":84},{"cell_type":"code","source":["#Investigating ICAO = '\\N'...\nif VALIDATE:\n  tmp = airports_tmp.select(\"*\").where(\"length(ICAO) == 2\")\n  display(tmp)\n\n  # in Brazil. No need to handle."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":85},{"cell_type":"code","source":["# DROP IATA='\\N' & ICAO= '\\N'\nairports_tmp = airports_tmp.filter(\"length(IATA) > 2\")\nairports_tmp = airports_tmp.filter(\"length(ICAO) > 2\")\nairports_tmp = airports_tmp.filter(\"IATA != 'NAN'\")\nairports_tmp.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[53]: 6071</div>"]}}],"execution_count":86},{"cell_type":"code","source":["# CREATE AIRPORTS CLEAN\nairports_clean = airports_tmp\nairports_clean.createOrReplaceTempView(\"airports_clean\")\nprint(f\"airports_clean: {airports_clean.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">airports_clean: 6071 rows\n</div>"]}}],"execution_count":87},{"cell_type":"code","source":["# VALIDATE AIRPORTS CLEAN\n# check for outliers\nif VALIDATE:\n  display(airports_clean.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":88},{"cell_type":"code","source":["# check for nulls/missing values\nif VALIDATE:\n  df = check_nulls_nans(airports_clean)\n  display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":89},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(airports_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":90},{"cell_type":"markdown","source":["### airlines_clean\n - eliminate duplicates\n - select relevant columns\n - evaluate and handle nulls"],"metadata":{}},{"cell_type":"code","source":["# check for duplicates\nif VALIDATE:\n  unique_id = [\"ORIGIN\", \"DEST\", \"OP_UNIQUE_CARRIER\", \"OP_CARRIER_FL_NUM\", \"FL_DATE\"]\n  w = Window.partitionBy(unique_id)\n  duplicate_flights = airlines_raw.select('*', f.count(\"*\")\\\n                      .over(w).alias('dupCount'))\\\n                      .where('dupCount > 1')\\\n                      .drop('dupCount')\n  display(duplicate_flights)\n  \n# OK => no duplicates  \n# if any are listed, drop them"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":92},{"cell_type":"code","source":["# ELIMINATE DUPLICATES/MULTIPLES\nunique_id = [\"ORIGIN\", \"DEST\", \"OP_UNIQUE_CARRIER\", \"OP_CARRIER_FL_NUM\", \"FL_DATE\"]\nw = Window.partitionBy(unique_id)\nairlines_tmp = airlines_raw.select('*', f.count(\"*\")\\\n                      .over(w).alias('dupCount'))\\\n                      .where('dupCount = 1')\\\n                      .drop('dupCount')\nairlines_tmp.createOrReplaceTempView(\"airlines_tmp\")\nairlines_tmp.count()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[59]: 31746811</div>"]}}],"execution_count":93},{"cell_type":"code","source":["# SELECT RELEVANT COLUMNS\nairline_relevant_columns = ['YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', \n                            'OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', \n                            'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR',\n                            'DEST', 'DEST_CITY_NAME', 'DEST_STATE_ABR',\n                            'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', \n                            'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', \n                            'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'AIR_TIME',\n                            'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', \n                            'CRS_ELAPSED_TIME', \n                            'FLIGHTS', 'holiday',\n                            'DISTANCE', 'DISTANCE_GROUP', \n                            'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']\n\n\nairlines_tmp = airlines_tmp.select(airline_relevant_columns) \nairlines_tmp.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[60]: 31746811</div>"]}}],"execution_count":94},{"cell_type":"code","source":["# check for outliers\nif VALIDATE:\n  display(airlines_tmp.describe())\n  \n# there appear to be no outliers in the \"primary\" fields we care about\n# TAXI_IN, TAXI_OUT, WHEEL_ON, WHEELS_OFF have figures a few 100s of minutes - seems fishy - careful if/when using as feature.\n# different counts for different fields => missing or null values"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":95},{"cell_type":"code","source":["# VALIDATE AIRLINES DATA\n# check for nulls/missing values\nif VALIDATE:\n  df = check_nulls_nans(airlines_tmp)\n  display(df)\n  \n# lot of fields with nulls and nans"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":96},{"cell_type":"code","source":["# CREATE AIRLINES CLEAN\nairlines_clean = airlines_tmp\nairlines_clean.createOrReplaceTempView(\"airlines_clean\")\nif VALIDATE:\n  print(f\"airlines_clean: {airlines_clean.count()} rows\") "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":97},{"cell_type":"code","source":["# VALIDATE AIRLINES CLEAN\n# check for outliers\nif VALIDATE:\n  display(airlines_clean.describe())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":98},{"cell_type":"code","source":["# check for nulls/missing values\nif VALIDATE:\n  df = check_nulls_nans(airlines_clean)\n  display(df)\n  \n# Should be all zeroes as all nulls have been accounted for"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":99},{"cell_type":"code","source":["# print schema\nif VERBOSE:\n  airlines_clean.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":100},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(airlines_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":101},{"cell_type":"markdown","source":["[Slide10-Sirak]"],"metadata":{}},{"cell_type":"markdown","source":["####Joins\n\n[NS7]As a part of joins, we\n - First joined airlines data with airports. The ORIGIN and DEST fields in airlines data mapped to the IATA field in the airports table\n - Next we joined weather data to airports data. The CALL_SIGN in weather data mapped to the ICAO of the airports data\n - Our big join consisted of joining the results of the above two joins.\n \nThe results of these joins were saved as our Silver Data.\n \n - We then joined on the aggregate delay data. This data was much smaller, since it has at most one record for each (airport, hour)"],"metadata":{}},{"cell_type":"markdown","source":["#### Join1"],"metadata":{}},{"cell_type":"markdown","source":["### airlines_silver (airlines join airports)\n - join airlines data with airports data, such that every airport has an associated ICAO code"],"metadata":{}},{"cell_type":"code","source":["# before the join, ensure that every ORIGIN in airlines_clean has a corresponding IATA, ICAO and Tz in airports_clean\n# missing values imply that:\n# 1 - there is no entry in airports.dat for ORIGIN airport, or\n# 2 - the ICAO is coded incorrectly for the airport (should not be the case if airports_clean is really clean)\nif VALIDATE:\n  origins = airlines_clean.select(\"ORIGIN\").distinct()\n  destinations = airlines_clean.select(\"DEST\").distinct()\n  airports_iata = airports_clean.select(\"IATA\").distinct()\n  \n  print(f\"flight origins: {origins.count()}\")\n  print(f\"flight destinations: {destinations.count()}\")\n  print(f\"airports with IATA: {airports_iata.count()}\")\n  print(f\"airports: {airports_clean.count()}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":106},{"cell_type":"code","source":["# Investigating further...\n# ORIGINs in airlines_clean with no matching IATA in airports_clean\nif VALIDATE:\n  missing = (airlines_clean.select(\"ORIGIN\").distinct()).subtract(airports_clean.select(\"IATA\").distinct())\n  print(f\"Origin airports with no matching IATAs: {missing.count()}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":107},{"cell_type":"code","source":["# Investigating further...\n# list of missing IATAs\nif VALIDATE:\n  flights = airlines_clean.join(missing, on=\"ORIGIN\", how=\"inner\")\n  display(flights.select(\"ORIGIN\", \"ORIGIN_CITY_NAME\").distinct())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":108},{"cell_type":"code","source":["# Investigating further...\n# flights with origins that have no matching IATA\nif VALIDATE:\n  flights = airlines_clean.join(missing, on=\"ORIGIN\", how=\"inner\")\n  print(f\"Flights with no matching IATA in airports_clean: {flights.count()}\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":109},{"cell_type":"markdown","source":["[Slide6-Max]"],"metadata":{}},{"cell_type":"markdown","source":["####Time Normalization \n\nWeather data has timestamps which are unambiguous representations of instants, independent of time zone, but times in the airlines dataset are awkwardly represented as decimal numbers and textual days: departure times are in the time zone of the origin airport and arrivals are in the time zone of the destination. We chose to normalize all these times to timestamps by converting them to UTC. We did this by using spark builtin conversion, but this concealed some details, such as daylight vs. standard time.\n\nWe used a publicly available airports database to get the time zone of airports and to assist in matching airline codes (IATA vs ICAO). This dataset covers almost all the airports that are referenced in the \"airlines\" (flights) dataset, but we needed to impute values for some missing airports."],"metadata":{}},{"cell_type":"code","source":["# FUNCTION TO JOIN AIRLINES AND AIRPORTS DATA\n# ensure LEFT JOIN between airlines and airport codes\n# we want to make sure every airport is associated with an IATA from airports\n\ndef make_airlines_join_airports(airlinesDF, airportsDF):\n  airport_codes_origin = airportsDF\n  airport_codes_dest = airportsDF\n\n  # update ICAO and universal timezone (Tz) and timezone offset (Timezone) for all origin aiports\n  df1 = ( airlinesDF.withColumnRenamed(\"ORIGIN\", \"IATA\")\n        .join(airport_codes_origin, on=\"IATA\", how=\"left\")\n        .withColumnRenamed(\"IATA\", \"ORIGIN\")\n        .withColumnRenamed(\"ICAO\", \"origin_icao\")\n        .withColumnRenamed(\"Tz\", \"origin_time_zone\")\n        .withColumnRenamed(\"Timezone\", \"origin_time_zone_offset\")\n        .withColumnRenamed(\"Name\", \"airport_name\")\n     )\n  \n  # update ICAO and universal timezone (Tz) and timezone offset (Timezone) for all destination aiports\n  df2 = ( df1.withColumnRenamed(\"DEST\", \"IATA\")\n        .join(airport_codes_dest, on=\"IATA\", how=\"left\")\n        .withColumnRenamed(\"IATA\", \"DEST\")\n        .withColumnRenamed(\"ICAO\", \"dest_icao\")\n        .withColumnRenamed(\"Tz\", \"dest_time_zone\")\n        .withColumnRenamed(\"Timezone\", \"dest_time_zone_offset\")\n     )\n  \n  # convert arrival and departure times to UTC format\n  # only crs_dep_time is needed\n  df3 = ( df2\n         .withColumn(\"crs_dep_time_utc\", f.to_utc_timestamp(f.format_string(\"%s %02d:%02d\", \n                                                    df2.FL_DATE, \n                                                    (df2.CRS_DEP_TIME / 100).cast(IntegerType()), \n                                                    df2.CRS_DEP_TIME % 100), \n                                                     df2.origin_time_zone))\n        .withColumn(\"crs_arr_time_utc\", f.to_utc_timestamp(f.format_string(\"%s %02d:%02d\", \n                                                   df2.FL_DATE, \n                                                   (df2.CRS_ARR_TIME / 100).cast(IntegerType()), \n                                                   df2.CRS_ARR_TIME % 100), \n                                                    df2.origin_time_zone))\n        .withColumn(\"dep_time_utc\", f.to_utc_timestamp(f.format_string(\"%s %02d:%02d\", \n                                                   df2.FL_DATE, \n                                                   (df2.DEP_TIME / 100).cast(IntegerType()), \n                                                   df2.DEP_TIME % 100), \n                                                    df2.origin_time_zone))\n        .withColumn(\"arr_time_utc\", f.to_utc_timestamp(f.format_string(\"%s %02d:%02d\", \n                                                   df2.FL_DATE, \n                                                   (df2.ARR_TIME / 100).cast(IntegerType()), \n                                                   df2.ARR_TIME % 100), \n                                                   df2.origin_time_zone))\n        )\n  \n  # add time buckets to all other time related fields\n  # Max: time buckets are only used for aggregation and join - they are not features. so we don't need to add buckets for all times\n  df4 = ( df3\n        .withColumn(\"CRS_DEP_HOUR\", (df3.CRS_DEP_TIME / 100).cast(IntegerType()))  # possible feature\n        .withColumn(\"CRS_ARR_HOUR\", (df3.CRS_ARR_TIME / 100).cast(IntegerType()))  # possible feature \n        # MAX: sch_dep_time_bucket could be used to join aggregates about scheduled traffic\n        .withColumn(\"sch_dep_time_bucket\", f.date_format(f.date_trunc(\"Hour\", df3.crs_dep_time_utc), \"yyyy-MM-dd HH:mm\"))\n        .withColumn(\"sch_arr_time_bucket\", f.date_format(f.date_trunc(\"Hour\", df3.crs_arr_time_utc), \"yyyy-MM-dd HH:mm\"))\n        .withColumn(\"act_dep_time_bucket\", f.date_format(f.date_trunc(\"Hour\", df3.dep_time_utc), \"yyyy-MM-dd HH:mm\"))\n        .withColumn(\"act_arr_time_bucket\", f.date_format(f.date_trunc(\"Hour\", df3.arr_time_utc), \"yyyy-MM-dd HH:mm\"))\n        # MAX: pred_time_bucket is used to join weather reports and aggregated previous delays\n        .withColumn(\"pred_time_bucket\", f.date_format(f.date_trunc(\"Hour\", df3.crs_dep_time_utc - f.expr(\"INTERVAL 2 HOUR\")), \"yyyy-MM-dd HH:mm\"))\n       )\n  \n  return df4\n      "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":112},{"cell_type":"code","source":["# CREATE AIRLINES_SILVER\nairlines_silver = make_airlines_join_airports(airlines_clean, airports_clean)\nairlines_silver.createOrReplaceTempView(\"airlines_silver\")\nif VALIDATE:\n  print(f\"airlines_silver: {airlines_silver.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":113},{"cell_type":"code","source":["# VALIDATE AIRLINES_SILVER\n# check for duplicates\nif VALIDATE:\n  unique_id = [\"ORIGIN\", \"DEST\", \"OP_UNIQUE_CARRIER\", \"OP_CARRIER_FL_NUM\", \"FL_DATE\"]\n  w = Window.partitionBy(unique_id)\n  display(airlines_silver.select('*', f.count(\"*\")\\\n                         .over(w).alias('dupCount'))\\\n                         .where('dupCount > 1')\\\n                         .drop('dupCount'))\n# OK => no duplicates  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":114},{"cell_type":"code","source":["# Evaluate whether every airport code has a valid ICAO\nif VALIDATE:\n  airlines_silver.select(\"*\").where(\"origin_icao is null\").count()\n  airlines_silver.select(\"*\").where(\"origin_time_zone is null\").count()\n\n# counts must be zero"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":115},{"cell_type":"code","source":["# check for nulls/nans\nif VALIDATE:\n  display(check_nulls_nans(airlines_silver))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":116},{"cell_type":"code","source":["# cross-check number of records with airlines_clean\nif VALIDATE:\n  silver_count = airlines_silver.count()  \n  clean_count = airlines_clean.count()\n  print(f\"silver_count {silver_count}\")\n  print(f\"clean_count {clean_count}\")  \n  assert(silver_count == clean_count)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":117},{"cell_type":"code","source":["# check outliers\n# Ensure no missing values - everything should be accounted for\nif VERBOSE:\n  display(airlines_silver.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":118},{"cell_type":"code","source":["# print schema\nif VERBOSE:\n  airlines_silver.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":119},{"cell_type":"code","source":["# display data\nif VERBOSE:\n  display(airlines_silver)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":120},{"cell_type":"markdown","source":["### weather_clean\n- filter for US\n- split fields where required\n- convert dates to UTC (are they already in?) \n- add time bucket\n- handle nulls\n- compact multiple rows for the same time bucket\n- interpolate missing measurements"],"metadata":{}},{"cell_type":"code","source":["# FILTER WEATHER DATA\n# US only \nweather_us = weather_raw.filter((f.col('report_type') == 'FM-15') & (f.col('call_sign') != '99999'))\nweather_us.createOrReplaceTempView(\"weather_us\")\nif VALIDATE:\n  print(f\"weather_us: {weather_us.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":122},{"cell_type":"code","source":["# verify counts of valid US stations (must equal ~2166)\nif VALIDATE:\n  display(weather_us.select(f.countDistinct(\"call_sign\")))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":123},{"cell_type":"markdown","source":["[Slide7-Max]"],"metadata":{}},{"cell_type":"markdown","source":["####Time Buckets\n\nSince there are many events in the system that we may want to join and aggregate, and these may all occur at slightly different timestamps, we introduced an idea of time \"buckets\". These are time intervals into which different events can be aggregated and by which aggregations can be joined. We chose hour-wide buckets, similar to the \"TIME_BLK\" fields of the airlines data, but unified to UTC time. These time buckets were to be used only as handles for data engineering and aggregation; they are not \"features\" in the ML sense - whereas the local TIME_BLK may be a feature that has predictive value.\n\nWe believed it is crucial to consider only data that was available at the prediction time, which is two hours before the flight departure time. We added a prediction time bucket to each flight, which we used as the join key to the weather data."],"metadata":{}},{"cell_type":"code","source":["# TRANSFORMATIONS\n# pre-split wind and temperature fields and handle missing values\n# apply scale refactoring as noted in ISD documentation\n# add time bucket \n# rename columns\n# collapse multiple measurements within a time bucket to a single one\n\ndef make_weather_clean(weatherDF):\n\n  wind_split = f.split(weatherDF.WND, ',')\n  tmp_split = f.split(weatherDF.TMP, ',')\n  dew_split = f.split(weatherDF.DEW, ',')\n    \n    \n  # Add one-hot encoded flags to indicate missing entries for wind, windangle, tmp and dew\n  # Wind angle is the exception, only considered missing if windspeed was also missing\n  weather_tmp = ( weatherDF  \n                .withColumn(\"CALL_SIGN\", trim(weatherDF.CALL_SIGN))\n                .withColumn(\"miss_windAngle\", f.when(wind_split.getItem(3) == \"999\", 1).otherwise(0))\n                .withColumn(\"windangle\", f.when(wind_split.getItem(3) == \"0000\", 0.)\n                                          .otherwise(wind_split.getItem(0).cast(\"float\")) )\n                .withColumn(\"miss_wnd\", f.when(wind_split.getItem(3) == \"999\", 1).otherwise(0))\n                .withColumn(\"WND\", wind_split.getItem(3).cast(\"float\") / 10.)\n                .withColumn(\"miss_tmp\", f.when(tmp_split.getItem(0) == \"+9999\", 1).otherwise(0))\n                .withColumn(\"TMP\", tmp_split.getItem(0).cast(\"float\") / 10.)\n                .withColumn(\"miss_dew\", f.when(dew_split.getItem(0) == \"+9999\", 1).otherwise(0))\n                .withColumn(\"DEW\", dew_split.getItem(0).cast(\"float\") / 10.)\n                .withColumn(\"CIG\", f.split(weatherDF.CIG, ',').getItem(0).cast(\"float\"))\n                .withColumn(\"VIS\", f.split(weatherDF.VIS, ',').getItem(0).cast(\"float\"))\n                .withColumn(\"time_bucket\", f.date_format(f.date_trunc(\"hour\", \"date\"), \"yyyy-MM-dd HH:mm\"))\n               )\n  \n  # collapse multiple weather rows for a given timebucket into a single one\n  unique_id = [\"CALL_SIGN\", \"time_bucket\"]\n  w = Window.partitionBy(unique_id).orderBy(f.desc('DATE'))\n  weather_tmp = weather_tmp.withColumn('Rank',f.dense_rank().over(w))\n  weather_tmp = weather_tmp.filter(weather_tmp.Rank == 1).drop(weather_tmp.Rank)\n  weather_tmp = weather_tmp.select(\"CALL_SIGN\", \"DATE\", \"windangle\", \"WND\", \"TMP\", \"DEW\",\n                                   \"miss_windAngle\", \"miss_wnd\", \"miss_tmp\", \"miss_dew\",\n                                   \"CIG\", \"VIS\", \"time_bucket\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\" )\n  return weather_tmp\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":126},{"cell_type":"code","source":["weather_tmp = make_weather_clean(weather_us)\nweather_tmp.createOrReplaceTempView(\"weather_tmp\")\nif VALIDATE:\n  print(f\"weather_clean: {weather_tmp.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":127},{"cell_type":"code","source":["# VALIDATE TRANSFORMATIONS\n# check for existence of multiple measurements with same time bucket - there should be none\nif VALIDATE:\n  unique_id = [\"CALL_SIGN\", \"time_bucket\"]\n  w = Window.partitionBy(unique_id)\n  display(weather_tmp.select('*', f.count(\"*\")\\\n                      .over(w).alias('dupCount'))\\\n                      .where('dupCount > 1')\\\n                      .drop('dupCount'))\n  \n  # OK => no duplicates or multiples => no need to collapse"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":128},{"cell_type":"code","source":["# check if there are any weather stations missing weather data that we care about i.e. CALL_SIGN, time_bucket, WND, windangle, CIG, VIS, TMP, DEW\nif VALIDATE:\n  df_tmp = weather_tmp.select('CALL_SIGN', 'time_bucket', 'WND', 'windangle', 'CIG', 'VIS', 'TMP', 'DEW',\n                              \"miss_windAngle\", \"miss_wnd\", \"miss_tmp\", \"miss_dew\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\")\n  df = check_nulls_nans(df_tmp)\n  display(df)\n  \n# All zeroes mean no nulls/nans => no need to correct for missing values"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":129},{"cell_type":"code","source":["# CREATE WEATHER_CLEAN\nweather_clean = weather_tmp\nweather_clean.createOrReplaceTempView(\"weather_clean\")\nif VALIDATE:\n  print(f\"weather_clean: {weather_clean.count()} rows\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":130},{"cell_type":"code","source":["# VALIDATE WEATHER CLEAN\n# check for outliers\nif VERBOSE:\n  display(weather_clean.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":131},{"cell_type":"code","source":["# check for nulls/nans\nif VALIDATE:\n  df_tmp = weather_clean.select('CALL_SIGN', 'time_bucket', 'WND', 'windangle', 'CIG', 'VIS', 'TMP', 'DEW', \n                                \"miss_windAngle\", \"miss_wnd\", \"miss_tmp\", \"miss_dew\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\")\n  df = check_nulls_nans(df_tmp)\n  display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":132},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(weather_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":133},{"cell_type":"markdown","source":["[Slide11-Sirak]"],"metadata":{}},{"cell_type":"markdown","source":["####Join2\n[NS8]"],"metadata":{}},{"cell_type":"markdown","source":["### weather_silver (airports join weather)\n- join weather data with airport data, such that every ICAO in the airports data is associated with a CALL_SIGN in weather data"],"metadata":{}},{"cell_type":"code","source":["# before joining, ensure no nulls in airports_clean\n# check for nulls/nans\nif VALIDATE:\n  display(check_nulls_nans(airports_clean))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":137},{"cell_type":"code","source":["# check to make sure there are no airports with ICAO = '\\N'\nif VALIDATE:\n  display(airports_clean.select(\"*\").where(\"length(ICAO) = 2\"))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":138},{"cell_type":"code","source":["# before joining, ensure no nulls in weather_clean\n# check for nulls/nans\nif VALIDATE:\n  display(check_nulls_nans(weather_clean))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":139},{"cell_type":"code","source":["# FUNCTION TO JOIN AIRPORTS WITH WEATHER\n# ensure RIGHT JOIN for weather.join(airports)\n\ndef make_weather_silver(weatherDF, airportsDF):\n    \"\"\" Join weather data with airports\n        Weather Data (CALL_SIGN)\n        Airports Data (ICAO)\n    \"\"\"\n       \n    # Join weather_raw to airport_codes based on ICAO\n    ## rename CALL_SIGN to ICAO to facilitate join\n    ## add a time_bucket field to capture the hour of recording\n    ## select relevant columns from weather_raw to minimize working data\n    ## inner join eliminates orphan weather records\n    df = ( weatherDF.withColumnRenamed(\"CALL_SIGN\", \"ICAO\")\n                  .withColumnRenamed(\"DATE\", \"date\")\n                  .withColumnRenamed(\"NAME\", \"station_name\")\n                  .withColumnRenamed(\"WND\", \"wind\")\n                  .withColumnRenamed(\"CIG\", \"cig\")\n                  .withColumnRenamed(\"VIS\", \"vis\")\n                  .withColumnRenamed(\"TMP\", \"tmp\")\n                  .withColumnRenamed(\"DEW\", \"dew\")\n                  .withColumnRenamed(\"LATITUDE\", \"LATITUDE\")\n                  .withColumnRenamed(\"LONGITUDE\", \"LONGITUDE\")\n                  .withColumnRenamed(\"ELEVATION\", \"ELEVATION\")\n                  .select(\"time_bucket\", \"ICAO\", \"wind\",  \"windangle\", \"cig\", \"vis\", \"tmp\", \"dew\",\n                          \"miss_windAngle\", \"miss_wnd\", \"miss_tmp\", \"miss_dew\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\")\n                  .join(airportsDF, on=\"ICAO\", how=\"right\")\n          )\n    return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":140},{"cell_type":"code","source":["# CREATE WEATHER_SILVER\nweather_silver = make_weather_silver(weather_clean, airports_clean)\nweather_silver.createOrReplaceTempView(\"weather_silver\")\nif VALIDATE:\n  print(f\"weather_silver: {weather_silver.count()} rows\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":141},{"cell_type":"code","source":["# check for duplicates\nif VALIDATE:\n  unique_id = [\"ICAO\", \"time_bucket\"]\n  w = Window.partitionBy(unique_id)\n  display(weather_silver.select('*', f.count(\"*\")\\\n                        .over(w).alias('dupCount'))\\\n                        .where('dupCount > 1')\\\n                        .drop('dupCount'))\n# OK => no duplicates \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":142},{"cell_type":"code","source":["# check for nulls/missing values - there should be none - should have all been handled by this point\nif VALIDATE:\n  df = check_nulls_nans(weather_silver)\n  display(df)  \n\n# all weather related fields must be zeroes\n# if weather related fields are non-zero, these imply airports with no weather stations"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":143},{"cell_type":"code","source":["# DESCRIPTIVE STATISTICS\nif VALIDATE:\n  # 2.63 minutes for 2015_2017\n  display(weather_silver.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":144},{"cell_type":"code","source":["# PRINT SCHEMA\nif VERBOSE:\n  weather_silver.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":145},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(weather_silver)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":146},{"cell_type":"markdown","source":["## Silver (joined) Dataset\n\nairports_silver joined with weather_silver"],"metadata":{}},{"cell_type":"markdown","source":["[Slide12-Sirak]"],"metadata":{}},{"cell_type":"markdown","source":["####Join3"],"metadata":{}},{"cell_type":"code","source":["# FUNCTION TO JOIN AIRLINES AND WEATHER \ndef make_airlines_join_weather(airlinesDF, weatherDF):\n  # join airlines_silver and weather_silver on origin airport (origin_icao) and prediction time bucket (time_bucket)\n  join_columns = [\"ICAO\",\"time_bucket\"]\n   \n  # join airlines and weather data on origin airport \n  df = ( airlinesDF\n        .withColumnRenamed(\"origin_icao\", \"ICAO\")\n        .withColumnRenamed(\"pred_time_bucket\", \"time_bucket\")\n        .join(weatherDF, join_columns, how=\"left\")\n        .withColumnRenamed(\"time_bucket\", \"pred_time_bucket\")\n        .withColumnRenamed(\"ICAO\", \"origin_icao\")\n       )\n  return df\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":150},{"cell_type":"code","source":["# (Madhukar) Identify common columns that need to be dropped after joining\ncolumns_to_drop = list(set(airlines_silver.columns).intersection(set(weather_silver.columns)))\nprint(\"Drop these duplicated columns\", columns_to_drop)\n\n# CREATE SILVER DATA\nsilver_df = make_airlines_join_weather(airlines_silver, weather_silver).drop(weather_silver.Name)\nsilver_df.createOrReplaceTempView(\"silver_df\")\nif VALIDATE:\n  airlines_silver_count = airlines_silver.count()  \n  silver_df_count = silver_df.count()\n  print(f\"airlines_silver_count {airlines_silver_count}\")\n  print(f\"silver_df_count {silver_df_count}\")  \n  assert(airlines_silver_count == silver_df_count)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Drop these duplicated columns [&#39;Name&#39;]\n</div>"]}}],"execution_count":151},{"cell_type":"code","source":["if VALIDATE:\n  query = f\"\"\"\nselect count(*) as count, \"silver_df\" as table from silver_df\nunion\nselect count(*), \"airlines_silver\" from airlines_silver\nunion\nselect count(*), \"weather_silver\" from weather_silver  \n\"\"\"\n  if VERBOSE:\n    print(query)\n  df = spark.sql(query)\n  display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":152},{"cell_type":"code","source":["# DESCRIPTIVE STATISTICS - all columns must have equal number of values\nif VERBOSE:\n  display(silver_df.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":153},{"cell_type":"code","source":["# check for nulls/missing values - there should be none - should have all been handled by this point\nif VALIDATE:\n  df = check_nulls_nans(silver_df)\n  display(df)  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":154},{"cell_type":"code","source":["# PRINT SCHEMA\nif VERBOSE:\n  silver_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":155},{"cell_type":"code","source":["# DISPLAY DATA\nif VERBOSE:\n  display(silver_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":156},{"cell_type":"markdown","source":["## Gold Datasets\nThese datasets refelect datasets ready for training our models\n* Gold_Train_Validate (2015-2017)\n* Gold_Test (2018)\n* once everything has been tested and is successful, we bring in 2019 via the same pipeline"],"metadata":{}},{"cell_type":"markdown","source":["[Slide8-Max]"],"metadata":{}},{"cell_type":"markdown","source":["####Airport Busyness Aggregation\n\nWe speculated that previous delays at an airport could contribute to further delays, so we developed aggregations that express delays, as evidenced by actual flights delayed, and planned busyness, as expressed by scheduled activity. Although these two kinds of measures are both aggregated by bucket for each airport, we were very cautious about how we join these aggregates. Scheduled activity in a given bucket can be joined with flights that depart in that bucket, since schedules are known in advance, but aggregate actual delay must be associated with the prediction time bucket, since at prediction time, we can only predict on the basis of the delays that have happened before prediction time. Moreover, since the full actual delays in minutes are not known until the flight actually takes off, we must aggregate actual delays by actual departure time. This might cause bad predictiveness in the case of very long delayed flights."],"metadata":{}},{"cell_type":"markdown","source":["##1. Departure Delay aggregates"],"metadata":{}},{"cell_type":"code","source":["def compute_airport_aggregated_delay(verbose=True):\n  \"\"\"\n  Create a time able that summarises average delays of flights that\n  departed in a given hour\n  \n  input: silver_df\n  \n  output: df with hourly delay aggregates:\n  prior_count_DEP_TIME, \n  prior_avg_DEP_DELAY, \n  prior_agg_DEP_DEL15, \n  prior_avg_TAXI_OUT, \n  prior_avg_CARRIER_DELAY, \n  prior_avg_WEATHER_DELAY, \n  prior_avg_NAS_DELAY,\n  prior_avg_SECURITY_DELAY, \n  prior_avg_LATE_AIRCRAFT_DELAY, \n  prior_DEP_OVERFLOW (this cant be calculated here b)\n  as difference between planned and actual departures\n  \"\"\"\n  \n  table_name = 'silver_df'\n  query = f\"\"\"\nselect ORIGIN, \n           act_dep_time_bucket as time_bucket,\n           count(DEP_TIME) as prior_count_DEP_TIME, \n           avg(DEP_DELAY) as prior_avg_DEP_DELAY,            \n           sum(DEP_DEL15) as prior_agg_DEP_DEL15, \n           avg(TAXI_OUT) as prior_avg_TAXI_OUT, \n           avg(CARRIER_DELAY) as prior_avg_CARRIER_DELAY, \n           avg(WEATHER_DELAY) as prior_avg_WEATHER_DELAY, \n           avg(NAS_DELAY) as prior_avg_NAS_DELAY, \n           avg(SECURITY_DELAY) as prior_avg_SECURITY_DELAY, \n           avg(LATE_AIRCRAFT_DELAY) as prior_avg_LATE_AIRCRAFT_DELAY\n         from {table_name}\n       group by ORIGIN, \n                act_dep_time_bucket \n    \"\"\"\n\n  if verbose:\n    print(query)\n    df = spark.sql(query)\n    return df\n\n  \nairport_aggregated_delay = compute_airport_aggregated_delay().cache()\nairport_aggregated_delay.createOrReplaceTempView(\"airport_aggregated_delay\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nselect ORIGIN, \n           act_dep_time_bucket as time_bucket,\n           count(DEP_TIME) as prior_count_DEP_TIME, \n           avg(DEP_DELAY) as prior_avg_DEP_DELAY,            \n           sum(DEP_DEL15) as prior_agg_DEP_DEL15, \n           avg(TAXI_OUT) as prior_avg_TAXI_OUT, \n           avg(CARRIER_DELAY) as prior_avg_CARRIER_DELAY, \n           avg(WEATHER_DELAY) as prior_avg_WEATHER_DELAY, \n           avg(NAS_DELAY) as prior_avg_NAS_DELAY, \n           avg(SECURITY_DELAY) as prior_avg_SECURITY_DELAY, \n           avg(LATE_AIRCRAFT_DELAY) as prior_avg_LATE_AIRCRAFT_DELAY\n         from silver_df\n       group by ORIGIN, \n                act_dep_time_bucket \n    \n</div>"]}}],"execution_count":161},{"cell_type":"code","source":["def compute_airport_aggregates(verbose=True):\n  \"\"\"\n  Create a time able that summarises aggregates of flights that\n  departed in a given hour\n  \n  input: silver_df\n  \n  output: df with hourly aggregates:\n  count_CRS_DEP_TIME\n  \"\"\"\n  \n  table_name = 'silver_df'\n  query = f\"\"\"\nselect ORIGIN, sch_dep_time_bucket,\n           count(CRS_DEP_TIME) as count_CRS_DEP_TIME\n         from {table_name}\n       group by ORIGIN, \n                sch_dep_time_bucket \n    \"\"\"\n\n  if verbose:\n    print(query)\n    df = spark.sql(query)\n    return df\n\n  \nairport_aggregates = compute_airport_aggregates().cache()\nairport_aggregates.createOrReplaceTempView(\"airport_aggregates\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nselect ORIGIN, sch_dep_time_bucket,\n           count(CRS_DEP_TIME) as count_CRS_DEP_TIME\n         from silver_df\n       group by ORIGIN, \n                sch_dep_time_bucket \n    \n</div>"]}}],"execution_count":162},{"cell_type":"markdown","source":["### Join departure aggregated delays with silver_df"],"metadata":{}},{"cell_type":"markdown","source":["[Slide13-Sirak]"],"metadata":{}},{"cell_type":"markdown","source":["####Join4"],"metadata":{}},{"cell_type":"code","source":["# join departure aggregates\ndef join_airport_agg_and_silver(airport_aggregated_delay, silver_df):\n  # rename columns to facilitate join\n  agg_df = (airport_aggregated_delay\n              .withColumnRenamed(\"time_bucket\", \"pred_time_bucket\")\n              .select(\"pred_time_bucket\", \"ORIGIN\", \"prior_count_DEP_TIME\",\n                      \"prior_avg_DEP_DELAY\", \"prior_agg_DEP_DEL15\", \"prior_avg_TAXI_OUT\",\n                      \"prior_avg_CARRIER_DELAY\", \"prior_avg_WEATHER_DELAY\", \"prior_avg_NAS_DELAY\",\n                      \"prior_avg_SECURITY_DELAY\", \"prior_avg_LATE_AIRCRAFT_DELAY\")\n            )\n              \n  res = (silver_df\n                  .withColumnRenamed(\"time_bucket\", \"pred_time_bucket\")\n                  .join(agg_df, on=['ORIGIN', 'pred_time_bucket'], how=\"left\") )\n  return res\n\ngold_df_tmp_joined = join_airport_agg_and_silver(airport_aggregated_delay, silver_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":166},{"cell_type":"code","source":["# add scheduled hourly aggregate count_CRS_DEP_TIME and join it\ndef add_hourly_agg(airport_aggregates, gold_df_tmp_joined):\n  # rename columns to facilitate join\n  hourly_agg_df = (airport_aggregates\n              .select(\"sch_dep_time_bucket\", \"ORIGIN\", \"count_CRS_DEP_TIME\")\n            )\n              \n  res = (gold_df_tmp_joined\n                  .join(hourly_agg_df, on=['ORIGIN', 'sch_dep_time_bucket'], how=\"left\") )\n  return res\n\ngold_df_tmp_joined = add_hourly_agg(airport_aggregates, gold_df_tmp_joined)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":167},{"cell_type":"code","source":["if VALIDATE:\n  gold_df_tmp_count = gold_df_tmp_joined.count()  \n  clean_count = airlines_clean.count()\n  print(f\"gold_df_tmp_count {gold_df_tmp_count}\")\n  print(f\"clean_count {clean_count}\")  \n  assert(gold_df_tmp_count == clean_count)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":168},{"cell_type":"markdown","source":["## 2. Tracking flight delays in previous leg using TAIL_NUM"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, lag, lead\n\nwindow_w_offset = (Window.partitionBy('TAIL_NUM', 'FL_DATE').orderBy('dep_time_utc'))\n\n# if null, impute 2 (so that prior_TN_DEP_DEL15 is OHE by 0, 1 and 2)\ngold_df_tmp_joined = (gold_df_tmp_joined.withColumn('prior_TN_DEP_DEL15', lag(col('DEP_DEL15'), 1, 2).over(window_w_offset))).cache()\n\n\n# introduce AIR_TIME_FLAG that flags 1 for >120min (that is, this flag is 0 for prior_TN_DEP_DEL15=[0,1], and 1 for prior_TN_DEP_DEL15=2)\ngold_df_tmp_joined = ( gold_df_tmp_joined\n                .withColumn(\"AIR_TIME_FLAG\", f.when(gold_df_tmp_joined.AIR_TIME>120, 1).otherwise(0))\n               )\n\n\ngold_df_tmp_joined.createOrReplaceTempView(\"gold_df_tmp_joined\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":170},{"cell_type":"code","source":["# replace prior_TN_DEP_DEL15 by 2 for flights <120min duration (so that prior_TN_DEP_DEL15 is OHE by 0, 1 and 2)\nfrom pyspark.sql import functions as F\ngold_df_tmp_joined = gold_df_tmp_joined.withColumn(\"prior_TN_DEP_DEL15\", F.when(F.col(\"AIR_TIME\")<120, 2).otherwise(F.col(\"prior_TN_DEP_DEL15\")))\ngold_df_tmp_joined.createOrReplaceTempView(\"gold_df_tmp_joined\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":171},{"cell_type":"markdown","source":["[Slide14-Rad]"],"metadata":{}},{"cell_type":"markdown","source":["### Feature Engineering\nInput feature attributes for our model were based mostly on intuition and work previously undertaken by researchers. We classified features into the following broad categories:\n\n**Spatial** characteristics: origin, destination\n\n**Temporal** characteristics: month, day of week, time of day\n\n**Flight Performance** characteristics: planned arrival, planned departure schedules \n\n**Weather** characteristics: represent external and environmental conditions\n\n**State of the System** characteristics: cumulative delays by category, number of flights departing in the hour, runway performance (taxi in/out) and late arrivals"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/feature%20selection.png?token=AKFL6AAT5A5LJQDRA4CWF427GVXAY' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/feature%20selection.png?token=AKFL6AAT5A5LJQDRA4CWF427GVXAY' >"]}}],"execution_count":174},{"cell_type":"markdown","source":["Most were readily available in the Silver data while the remaining new features were aggregated separately and joined with the Silver data. As previously pointed out, time buckets were used to both generate these counts as well as append them as features of each of the flights.\n\nWe also noted that correlated and irrelevant features may provide model overfitting and decrease prediction performance, and analysing feature importance may enable us to discriminate and study the most impactful features. Although we did not have a chance to implement this in the current approach, we plan to do so in our next iteration. \n\nNext, all categorical features were identified and converted to strings and all numeric variables were converted to doubles and the flights along with selected features were saved as our GOLD data."],"metadata":{}},{"cell_type":"markdown","source":["#### *Convert Categorical Variables to Strings, and Numericals to Doubles"],"metadata":{}},{"cell_type":"code","source":["# identify all categorical columns and convert them to string, and convert all numerical columns to doubles\n\nfull_categorical_features = ['ORIGIN', 'sch_dep_time_bucket', 'ICAO', 'pred_time_bucket', 'DEST', 'YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_FIPS', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST_CITY_NAME', 'DEST_STATE_ABR', 'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'CRS_DEP_TIME', 'DEP_TIME',  'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'WHEELS_OFF', 'WHEELS_ON', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED',  'DISTANCE_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'airport_name',  'origin_time_zone', 'dest_icao', 'dest_time_zone', 'crs_dep_time_utc', 'crs_arr_time_utc', 'dep_time_utc', 'arr_time_utc', 'sch_arr_time_bucket', 'act_dep_time_bucket', 'act_arr_time_bucket', 'station_name', 'IATA', 'Name', 'Timezone', 'Tz', 'ORIGIN-DEST', 'prior_TN_DEP_DEL15', 'holiday', 'AIR_TIME_FLAG'\n                            ]\n\nfull_numerical_features = ['DEP_DELAY', 'DEP_DELAY_NEW', 'TAXI_OUT', 'TAXI_IN', 'ARR_DELAY', 'ARR_DELAY_NEW', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'FLIGHTS', 'DISTANCE', 'origin_time_zone_offset', 'dest_time_zone_offset', 'CRS_DEP_HOUR', 'CRS_ARR_HOUR', 'wind', 'windangle', 'cig', 'vis', 'tmp', 'dew', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'count_CRS_DEP_TIME', 'prior_count_DEP_TIME', 'prior_agg_DEP_DEL15', 'prior_avg_DEP_DELAY', 'prior_avg_TAXI_OUT', 'prior_avg_CARRIER_DELAY', 'prior_avg_WEATHER_DELAY', 'prior_avg_NAS_DELAY', 'prior_avg_SECURITY_DELAY', 'prior_avg_LATE_AIRCRAFT_DELAY', \"miss_windAngle\", \"miss_wnd\", \"miss_tmp\", \"miss_dew\", 'prior_dep_agg_w_overflow'\n                          ]\n\nfrom pyspark.sql.functions import expr, col, column\n\ndef cast_features(df, full_categorical_features, full_numerical_features):\n  for cat_feature in full_categorical_features:\n    if cat_feature in df.columns:\n      df = (df.withColumn(cat_feature, col(cat_feature).cast(\"string\")))\n    \n  for num_feature in full_numerical_features:\n    if num_feature in df.columns:\n      df = (df.withColumn(num_feature, col(num_feature).cast(\"double\")))\n  \n  return df\n\ngold_df_tmp_cast = cast_features(gold_df_tmp_joined, full_categorical_features, full_numerical_features)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":177},{"cell_type":"code","source":["if VERBOSE :\n  gold_df_tmp_cast.count()\n  gold_df_tmp_cast.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":178},{"cell_type":"code","source":["# identify the features that need to be saved in gold_df\nfeature_list = ['ORIGIN', 'DEST', 'ORIGIN-DEST', \n                'YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'CRS_DEP_HOUR', \n                'FL_DATE', 'TAIL_NUM', 'FLIGHTS', 'DISTANCE', 'DISTANCE_GROUP',\n                'wind', 'windangle', 'cig', 'vis', 'tmp', 'dew', \"miss_windAngle\", \"miss_wnd\", \"miss_tmp\", \"miss_dew\", \n                'LATITUDE', 'LONGITUDE', 'ELEVATION', 'holiday',\n                'DEP_DEL15', \n                'count_CRS_DEP_TIME', 'prior_count_DEP_TIME', 'prior_agg_DEP_DEL15', \n                'prior_avg_DEP_DELAY', 'prior_avg_TAXI_OUT', \n                'prior_avg_CARRIER_DELAY', 'prior_avg_WEATHER_DELAY', 'prior_avg_NAS_DELAY', \n                'prior_avg_SECURITY_DELAY', 'prior_avg_LATE_AIRCRAFT_DELAY', 'prior_TN_DEP_DEL15', 'AIR_TIME_FLAG'\n               ]\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":179},{"cell_type":"code","source":["def df_trim(old_df, feature_list, keep_features=True):\n  \"\"\"\n  returns trimmed DF that only keeps (if True, else drops) columns in features_list\n  \n  Input: original dataframe, list of features, keep features=True will keep only these features\n  Output: trimmed dataframe\n  \"\"\"\n\n  if keep_features:\n    del_feature_list = set(list(silver_df.columns)) - set(feature_list)\n  else:\n    del_feature_list = feature_list\n    \n  trimmed_df = old_df.drop(*del_feature_list) # drop all columns in del_feature_list\n  return trimmed_df\n\ngold_df = df_trim(gold_df_tmp_cast, feature_list, True).cache() # this is needed to get rid of columns (like DIV1 etc) that have all nulls\ngold_df.createOrReplaceTempView(\"gold_df\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":180},{"cell_type":"code","source":["if VERBOSE :\n  gold_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":181},{"cell_type":"markdown","source":["#### *Save Gold Data"],"metadata":{}},{"cell_type":"code","source":["# SAVE GOLD DATASET TO DISK\ngold_data_path = f\"/team7/{user_initials}/{DATASETS}/gold_dataset\"\n\ndef persist_gold(df):\n  # SAVE GOLD DATASET TO DISK\n  (df.write.mode(\"overwrite\").format(\"parquet\").save(gold_data_path))\n\ndef init_gold_from_disk():\n  # IMPORT GOLD DATA FROM DISK\n  print(\"importing gold data from disk\")\n  df = sqlContext.read.option(\"header\", \"true\").parquet(f\"dbfs:\" + gold_data_path)\n  return df\n\nif not FORCE_COMPUTE_ALL and file_exists(gold_data_path):\n  gold_df = init_gold_from_disk()\n  print(\"Gold data already exists at {}!\".format(gold_data_path))\n\ngold_df.createOrReplaceTempView(\"gold_df\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":183},{"cell_type":"markdown","source":["# Model Building"],"metadata":{}},{"cell_type":"markdown","source":["[Slide15-Madhukar]"],"metadata":{}},{"cell_type":"markdown","source":["###Algorithm Exploration\n[NS12]We tried Logistic Regression (LR), Random Forest (RF) and Gradient Boosted Trees (GBT), and finally chose LR and GBT. \n\nLogistic regression returns probabilities of belonging to class 0 or class 1, hence one could set an appropriate threshold that would maximize precision (as opposed to recall). For instance, one could set the decision threshold at a probability of 0.9 for the label to be deemed positive (1, or delay in our case). While we expect logistic regression to be decently performant, it could suffer greatly from overfitting, and hence result in a high variance. \n\nBy choosing ensemble methods like Random Forests and Gradient Boosted Trees, we can trade bias for variance (that is, we want a model that is reasonably robust to incoming/updated training data).\nWhile we really wanted to implement LightGBM and XGBoost, these were not readily available to us - the former could not be loaded onto the cluster due to issues with the third party Microsoft Azure library, while the latter was available only in Scala. As a compromise, we chose GBT. \n\nIn order to further improve model performance, we used stacking by taking the predictions of logistic regression and GBT and passing them through another meta-estimator GBT. We found that the performance of the stacked model was the best overall, followed by GBT and then LR."],"metadata":{}},{"cell_type":"markdown","source":["- Tried Logistic Regression (LR), Random Forest (RF) and Gradient Boosted Trees (GBT)\n- **Trade bias for variance using ensemble methods** like GBTs\n- LightGBM and XGBoost were not readily available to us\n- In order to further improve model performance, we used **stacking of LR and GBT with GBT as meta-estimator**"],"metadata":{}},{"cell_type":"markdown","source":["d ###Algorithm Implementation"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/Model%20Pipeline.png?token=AFP5XG3ZQLVXWJHGK3A5HTS7GWM4U' width=900 height=350>\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/Model%20Pipeline.png?token=AFP5XG3ZQLVXWJHGK3A5HTS7GWM4U' width=900 height=350>"]}}],"execution_count":189},{"cell_type":"markdown","source":["GBT is an effective ensemble learning algorithm based on the idea of boosting. It’s easier to explain for the case of regression, than for classification. A first decision tree regressor is built. Then, its residuals are used to train a second tree (after multiplying the residuals with a learning rate). Then a third tree is built based on the new residuals, and so on. Boosting comes from the learning rate, which is a hyperparameter, and helps to reduce bias. Thus GBT can greatly overfit leading to high variance, and therefore the depth of trees as well as the total number of boosted trees are used as hyperparameters to improve the variance. GBT for classification works on the same principles, but deals with log-likelihood maximization instead of residual minimization.\n\nCategorical features were one-hot encoded, whereby they were first indexed and then converted to n dimensional vectors (for a category consisting of n levels). \n\nWe experimented with standardization techniques using StandardScaler and RobustScaler, but found they did not really help much. This may be because use of feature scaling in the algorithms we chose (LR and GBT) were already robust to features lying in different ranges.\n\nWe chose 2015-2018 as our training dataset, and the 2019 data for testing. We considered large periods of time to ensure that the inferred model is able to predict delays due to almost every condition except for rare events not captured in training data.\n\nSince the flights with DEP_DEL15 flag of 1 were only ~18% of the dataset, there was considerable data imbalance, and hence there was a great chance of getting the minor class predictions incorrect (due to the low training examples for that class). To overcome this, we utilized class weights option in the logistic regression, but saw only a very small improvement (suggesting that the variance in our models was very small already). We looked to implement SMOTE, but found it was not readily available in PySpark.\n\nWe used GridSearch for hyperparameter tuning, and implemented TrainingValidationSplit (TSV) with 0.75/0.25 ratio during initial stages of our model building before switching to CrossValidation (5-fold CV). We found that TSV gave very similar prediction scores as 5-fold CV, with the added advantage of being able to run the code in practical times (~3hrs compared to >10 hrs).\n\nWhile our goal was to improve precision, we chose areaUnderPR as our metric of choice for optimization of the tradeoff between precision and recall. One can get a Precision of perfect 1.0 by predicting 0 for all examples! Clearly, this is undesired. For an imbalanced dataset as in our case, an F-beta score may be more appropriate as it provides a configurable mixture of precision vs recall . Specifically, F0.5 score with more emphasis on precision would be a metric of choice if our business requirement was to minimize false positives. However, for practical use, F1 score with balance weightage given to precision and recall may be a healthy compromise, and is one of the key scores that we rely on. \n\nhttps://machinelearningmastery.com/fbeta-measure-for-machine-learning/#:~:text=The%20choice%20of%20the%20beta,measure%20or%20the%20F1%2Dscore."],"metadata":{}},{"cell_type":"markdown","source":["#### * Split Gold Data into Train-Validate, Test"],"metadata":{}},{"cell_type":"code","source":["# drop columns have do not have multiple values to avoid erroring out while indexing\nif DATASETS == 'Toy':\n  gold_df = gold_df.drop(\"YEAR\").drop(\"QUARTER\") \nelif DATASETS == '2015':\n  gold_df = gold_df.drop(\"YEAR\")\n\ngold_df = gold_df.na.drop() \n\nif DATASETS == '2015-2019':\n  train = gold_df.filter((gold_df.YEAR== '2015') | (gold_df.YEAR== '2016') | (gold_df.YEAR== '2017')| (gold_df.YEAR== '2018'))\n  test = gold_df.filter((gold_df.YEAR== '2019'))\nelse:\n  train, test = gold_df.randomSplit([.80, 0.20], seed=104)\n  train.createOrReplaceTempView(\"train\")\n  test.createOrReplaceTempView(\"test\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":192},{"cell_type":"code","source":["if VERBOSE:\n  display(gold_df.describe().cache())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":193},{"cell_type":"markdown","source":["#### *Balance Data: Generate Class Weights\n\n- Used class weights for Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["# Uncomment to generate class weights\n# dataset_size=float(train.count())\n# numPositives=train.select(\"DEP_DEL15\").where('DEP_DEL15 == 1').count()\n# per_ones=(float(numPositives)/float(dataset_size))*100\n# numNegatives=float(dataset_size-numPositives)\n# print('The number of ones are {}'.format(numPositives))\n# print('Percentage of ones are {}'.format(per_ones))\n# BalancingRatio= numNegatives/dataset_size\n# print('BalancingRatio = {}'.format(BalancingRatio))\n\n# pre-calculated values here to save time\nif DATASETS == \"Toy\":\n  BalancingRatio = 0.7517532554108707\nelif DATASETS == \"2015\":\n  BalancingRatio = 0.7736956152970962\nelif DATASETS == \"2015_2017\":\n  BalancingRatio = 0.7794535029323947\nelse:\n  BalancingRatio = 0.7792115713310707  \ntrain=train.withColumn(\"classWeights\", F.when(train.DEP_DEL15 == 1,BalancingRatio).otherwise(1-BalancingRatio))\ntrain.select(\"classWeights\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+\n       classWeights|\n+-------------------+\n0.22078842866892934|\n0.22078842866892934|\n 0.7792115713310707|\n 0.7792115713310707|\n0.22078842866892934|\n+-------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":195},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import RobustScaler\nfrom pyspark.ml.feature import RFormula\nfrom pyspark.ml.classification import LogisticRegression\n\n\"\"\"\n# RFormula treats strings as categorical features and one-hot encodes them\n# RFormula creates two columns \"label\" and \"features\"\n# \"features\" column is a big long string of all features\n\"\"\"\n\n# add in QUARTER, YEAR if 2015 or larger datasets, resp.\nrForm_Toy = (RFormula(formula = \"DEP_DEL15 ~ DEST + ORIGIN + MONTH + DAY_OF_MONTH + DAY_OF_WEEK + DISTANCE + DISTANCE_GROUP + CRS_DEP_HOUR + wind + windangle + cig + vis + tmp + dew + LATITUDE + LONGITUDE + ELEVATION + count_CRS_DEP_TIME  + prior_count_DEP_TIME + prior_agg_DEP_DEL15 + prior_avg_DEP_DELAY + prior_avg_TAXI_OUT + prior_avg_CARRIER_DELAY + prior_avg_WEATHER_DELAY + prior_avg_NAS_DELAY + prior_avg_SECURITY_DELAY + prior_avg_LATE_AIRCRAFT_DELAY + holiday + miss_windAngle + miss_wnd + miss_tmp + miss_dew + AIR_TIME_FLAG\", featuresCol=\"num_ohe_features\", labelCol=\"label\").setHandleInvalid(\"skip\"))\n\nrForm_2015 = (RFormula(formula = \"DEP_DEL15 ~ QUARTER + DEST + ORIGIN + MONTH + DAY_OF_MONTH + DAY_OF_WEEK + DISTANCE + DISTANCE_GROUP + CRS_DEP_HOUR + wind + windangle + cig + vis + tmp + dew + LATITUDE + LONGITUDE + ELEVATION + count_CRS_DEP_TIME  + prior_count_DEP_TIME + prior_agg_DEP_DEL15 + prior_avg_DEP_DELAY + prior_avg_TAXI_OUT + prior_avg_CARRIER_DELAY + prior_avg_WEATHER_DELAY + prior_avg_NAS_DELAY + prior_avg_SECURITY_DELAY + prior_avg_LATE_AIRCRAFT_DELAY + holiday + miss_windAngle + miss_wnd + miss_tmp + miss_dew + AIR_TIME_FLAG\", featuresCol=\"num_ohe_features\", labelCol=\"label\").setHandleInvalid(\"skip\"))\n\nrForm = (RFormula(formula = \"DEP_DEL15 ~ YEAR + QUARTER + DEST + ORIGIN + MONTH + DAY_OF_MONTH + DAY_OF_WEEK + DISTANCE + DISTANCE_GROUP + CRS_DEP_HOUR + wind + windangle + cig + vis + tmp + dew + LATITUDE + LONGITUDE + ELEVATION + count_CRS_DEP_TIME  + prior_count_DEP_TIME + prior_agg_DEP_DEL15 + prior_avg_DEP_DELAY + prior_avg_TAXI_OUT + prior_avg_CARRIER_DELAY + prior_avg_WEATHER_DELAY + prior_avg_NAS_DELAY + prior_avg_SECURITY_DELAY + prior_avg_LATE_AIRCRAFT_DELAY + holiday + miss_windAngle + miss_wnd + miss_tmp + miss_dew + AIR_TIME_FLAG\", featuresCol=\"num_ohe_features\", labelCol=\"label\").setHandleInvalid(\"skip\"))  \n  \nif DATASETS == \"Toy\":\n  num_ohe_stage = rForm_Toy\nelif DATASETS == '2015':\n  num_ohe_stage = rForm_2015\nelse:\n  num_ohe_stage = rForm\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":196},{"cell_type":"markdown","source":["## Model Implementations"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import TrainValidationSplit\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nimport pyspark.sql.functions as F\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.sql.types import FloatType"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":198},{"cell_type":"code","source":["# Helper functions to save the \"best model\" to disk\ngold_data_path = f\"/team7/{user_initials}/{DATASETS}/gold_dataset\"\n\ndef persist_model(tvsFitted, model_name):\n  model_path = f\"/team7/MR/{DATASETS}/{model_name}\"\n  best_model = tvsFitted.bestModel\n  best_model.write().overwrite().save(model_path)\n\n# load model from disk\ndef load_model(model_name):\n  model_path = f\"/team7/MR/{DATASETS}/{model_name}\"\n  loaded_model = PipelineModel.load(model_path)\n  return loaded_model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":199},{"cell_type":"markdown","source":["###Logistic Regression Pipeline"],"metadata":{}},{"cell_type":"code","source":["# logistic regression takes in \"label\" and \"features\"\nlr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\").setWeightCol(\"classWeights\")\nlr_pipeline = Pipeline(stages=[num_ohe_stage, VectorAssembler(inputCols=[\"num_ohe_features\"], outputCol=\"features\"), lr])\nlr_params = (ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n                            .addGrid(lr.regParam, [0.1, 2.0])\n                            .build())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":201},{"cell_type":"markdown","source":["### Gradient Boosted Trees (GBT) Pipeline"],"metadata":{}},{"cell_type":"code","source":["gbt = GBTClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\")\ngbt_pipeline = Pipeline(stages=[num_ohe_stage, VectorAssembler(inputCols=[\"num_ohe_features\"], outputCol=\"features\"), gbt])\ngbt_params = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 30])\n             .addGrid(gbt.maxIter, [10, 15])\n             .build())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":203},{"cell_type":"code","source":["def run_and_persist_best_model(model_name, params, pipeline, data):\n  evaluator = (BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\")\n                                            .setRawPredictionCol(\"rawPrediction\")\n                                            .setLabelCol(\"label\"))\n  crossval = (CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=params,\n                          evaluator=evaluator,\n                          numFolds=5))  \n\n  # train the model\n  crossvalFitted = crossval.fit(data)\n  persist_model(crossvalFitted, model_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":204},{"cell_type":"code","source":["if PERSIST_MODELS:\n  run_and_persist_best_model(\"lr\", lr_params, lr_pipeline, train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":205},{"cell_type":"code","source":["if PERSIST_MODELS:\n  run_and_persist_best_model(\"gbt\", gbt_params, gbt_pipeline, train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":206},{"cell_type":"markdown","source":["### Logistic Regression Results"],"metadata":{}},{"cell_type":"code","source":["# # evaluate the model\nlr_transform_test = load_model(\"lr\").transform(test)\n\nareaUnderROC = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").evaluate(lr_transform_test)\nareaUnderPR = BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").evaluate(lr_transform_test)\nf1 = MulticlassClassificationEvaluator().setMetricName(\"f1\").evaluate(lr_transform_test)\nweightedPrecision = MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\").evaluate(lr_transform_test)\nweightedRecall = MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\").evaluate(lr_transform_test)\naccuracy = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(lr_transform_test)\n\nprint(\"areaUnderROC = \", areaUnderROC)\nprint(\"areaUnderPR = \", areaUnderPR)\nprint(\"f1 = \", f1)\nprint(\"weightedPrecision = \", weightedPrecision)\nprint(\"weightedRecall = \", weightedRecall)\nprint(\"accuracy = \", accuracy)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">areaUnderROC =  0.678005656860986\nareaUnderPR =  0.3962256283309613\nf1 =  0.678342703717906\nweightedPrecision =  0.737480681193688\nweightedRecall =  0.6514884789007018\naccuracy =  0.6514884789007018\n</div>"]}}],"execution_count":208},{"cell_type":"code","source":["metrics = lr_transform_test.select(\"label\", \"prediction\").rdd.map(tuple)\nmetrics = MulticlassMetrics(metrics)\nprint(\"Confusion Matrix:\", metrics.confusionMatrix().toArray())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Confusion Matrix: [[2031753.  354330.]\n [1003374.  506265.]]\n</div>"]}}],"execution_count":209},{"cell_type":"code","source":["y_true = lr_transform_test.select(\"label\").collect()\ny_pred = lr_transform_test.select(\"prediction\").collect()\nprint(classification_report(y_true, y_pred))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n         0.0       0.85      0.67      0.75   3035127\n         1.0       0.34      0.59      0.43    860595\n\n    accuracy                           0.65   3895722\n   macro avg       0.59      0.63      0.59   3895722\nweighted avg       0.74      0.65      0.68   3895722\n\n</div>"]}}],"execution_count":210},{"cell_type":"markdown","source":["### GBT Results"],"metadata":{}},{"cell_type":"code","source":["# # evaluate the model\ngbt_transform_test = load_model(\"gbt\").transform(test)\n\nareaUnderROC = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").evaluate(gbt_transform_test)\nareaUnderPR = BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").evaluate(gbt_transform_test)\nf1 = MulticlassClassificationEvaluator().setMetricName(\"f1\").evaluate(gbt_transform_test)\nweightedPrecision = MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\").evaluate(gbt_transform_test)\nweightedRecall = MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\").evaluate(gbt_transform_test)\naccuracy = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(gbt_transform_test)\n\nprint(\"areaUnderROC = \", areaUnderROC)\nprint(\"areaUnderPR = \", areaUnderPR)\nprint(\"f1 = \", f1)\nprint(\"weightedPrecision = \", weightedPrecision)\nprint(\"weightedRecall = \", weightedRecall)\nprint(\"accuracy = \", accuracy)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">areaUnderROC =  0.6811002316022879\nareaUnderPR =  0.4070982086444518\nf1 =  0.7237228937142821\nweightedPrecision =  0.7594576887089426\nweightedRecall =  0.7887872902635249\naccuracy =  0.788787290263525\n</div>"]}}],"execution_count":212},{"cell_type":"code","source":["metrics = gbt_transform_test.select(\"label\", \"prediction\").rdd.map(tuple)\nmetrics = MulticlassMetrics(metrics)\nprint(\"Confusion Matrix:\", metrics.confusionMatrix().toArray())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Confusion Matrix: [[2984656.  772355.]\n [  50471.   88240.]]\n</div>"]}}],"execution_count":213},{"cell_type":"code","source":["y_true = gbt_transform_test.select(\"label\").collect()\ny_pred = gbt_transform_test.select(\"prediction\").collect()\nprint(classification_report(y_true, y_pred))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n         0.0       0.79      0.98      0.88   3035127\n         1.0       0.64      0.10      0.18    860595\n\n    accuracy                           0.79   3895722\n   macro avg       0.72      0.54      0.53   3895722\nweighted avg       0.76      0.79      0.72   3895722\n\n</div>"]}}],"execution_count":214},{"cell_type":"markdown","source":["## Stacking"],"metadata":{}},{"cell_type":"code","source":["gbt_transform_train = load_model(\"gbt\").transform(train)\nlr_transform_train = load_model(\"lr\").transform(train)\ngbt_transform_train = gbt_transform_train.withColumnRenamed(\"prediction\", \"gbt_prediction\").withColumn(\"gbt_prediction\", col(\"gbt_prediction\").cast(\"string\"))\nlr_transform_train = lr_transform_train.withColumnRenamed(\"prediction\", \"lr_prediction\").withColumn(\"lr_prediction\", col(\"lr_prediction\").cast(\"string\"))\ngbt_transform_train.createOrReplaceTempView(\"gbt_transform_train\")\nlr_transform_train.createOrReplaceTempView(\"lr_transform_train\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":216},{"cell_type":"code","source":["stacked_train_predictions_df = lr_transform_train.select('ORIGIN', 'DEST', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'TAIL_NUM', 'DEP_DEL15', 'FLIGHTS', 'CRS_DEP_HOUR','lr_prediction').join(gbt_transform_train.select('ORIGIN', 'DEST', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'TAIL_NUM', 'DEP_DEL15', 'FLIGHTS', 'CRS_DEP_HOUR', 'gbt_prediction'), how=\"left\", on=['ORIGIN', 'DEST', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'TAIL_NUM', 'DEP_DEL15', 'FLIGHTS', 'CRS_DEP_HOUR']).select(\"DEP_DEL15\", \"lr_prediction\", \"gbt_prediction\")\n\nstacked_train_predictions_df = (stacked_train_predictions_df.withColumn(\"DEP_DEL15\", col(\"DEP_DEL15\").cast(\"string\")))\nstacked_train_predictions_df.createOrReplaceTempView(\"stacked_train_predictions_df\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":217},{"cell_type":"code","source":["stacked_train_predictions_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- DEP_DEL15: string (nullable = true)\n-- lr_prediction: string (nullable = false)\n-- gbt_prediction: string (nullable = true)\n\n</div>"]}}],"execution_count":218},{"cell_type":"markdown","source":["#### Build Stacking Pipeline"],"metadata":{}},{"cell_type":"code","source":["stacked_rForm = (RFormula(formula = \"DEP_DEL15 ~ lr_prediction + gbt_prediction\").setHandleInvalid(\"skip\"))\nmeta_gbt = GBTClassifier()#.setLabelCol(\"label\").setFeaturesCol(\"features\")\nstacked_pipeline = Pipeline(stages=[stacked_rForm, meta_gbt])\n\nmeta_params = (ParamGridBuilder()\n             .addGrid(meta_gbt.maxDepth, [2, 4, 6])\n             .addGrid(meta_gbt.maxBins, [20, 30])\n             .addGrid(meta_gbt.maxIter, [10, 15])\n             .build())\n\nif PERSIST_MODELS: \n  run_and_persist_best_model(\"stacked_best_model\", meta_params, stacked_pipeline, stacked_train_predictions_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":220},{"cell_type":"markdown","source":["### Evaluation of Test Dataset using Stacking Estimator"],"metadata":{}},{"cell_type":"code","source":["# generate lr_prediction and gbt_prediction for test dataset using lr_best_model and gbt_best_model\n\ngbt_transform_test = load_model(\"gbt\").transform(test)\nlr_transform_test = load_model(\"lr\").transform(test)\ngbt_transform_test = gbt_transform_test.withColumnRenamed(\"prediction\", \"gbt_prediction\").withColumn(\"gbt_prediction\", col(\"gbt_prediction\").cast(\"string\"))\nlr_transform_test = lr_transform_test.withColumnRenamed(\"prediction\", \"lr_prediction\").withColumn(\"lr_prediction\", col(\"lr_prediction\").cast(\"string\"))\ngbt_transform_test.createOrReplaceTempView(\"gbt_transform_test\")\nlr_transform_test.createOrReplaceTempView(\"lr_transform_test\")\n\n# create new_test_df with lr_test_prediction and gbt_test_prediction\nstacked_test_predictions_df = lr_transform_test.select('ORIGIN', 'DEST', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'TAIL_NUM', 'DEP_DEL15', 'FLIGHTS', 'CRS_DEP_HOUR','lr_prediction').join(gbt_transform_test.select('ORIGIN', 'DEST', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'TAIL_NUM', 'DEP_DEL15', 'FLIGHTS', 'CRS_DEP_HOUR', 'gbt_prediction'), how=\"left\", on=['ORIGIN', 'DEST', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'TAIL_NUM', 'DEP_DEL15', 'FLIGHTS', 'CRS_DEP_HOUR']).select(\"DEP_DEL15\", \"lr_prediction\", \"gbt_prediction\")\n\nstacked_test_predictions_df = stacked_test_predictions_df.withColumn(\"DEP_DEL15\", col(\"DEP_DEL15\").cast(\"string\"))#.withColumnRenamed(\"DEP_DEL15\", \"label\")\nstacked_test_predictions_df.createOrReplaceTempView(\"stacked_test_predictions_df\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":222},{"cell_type":"code","source":["# load the model and transform the stacked_test_predictions_df\nmodel_name = \"stacked_best_model\"\nloaded_prediction_and_labels = load_model(model_name).transform(stacked_test_predictions_df)\n\nareaUnderROC = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").evaluate(loaded_prediction_and_labels)\nareaUnderPR = BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").evaluate(loaded_prediction_and_labels)\nf1 = MulticlassClassificationEvaluator().setMetricName(\"f1\").evaluate(loaded_prediction_and_labels)\nweightedPrecision = MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\").evaluate(loaded_prediction_and_labels)\nweightedRecall = MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\").evaluate(loaded_prediction_and_labels)\naccuracy = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(loaded_prediction_and_labels)\n\nprint(\"areaUnderROC = \", areaUnderROC)\nprint(\"areaUnderPR = \", areaUnderPR)\nprint(\"f1 = \", f1)\nprint(\"weightedPrecision = \", weightedPrecision)\nprint(\"weightedRecall = \", weightedRecall)\nprint(\"accuracy = \", accuracy)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">areaUnderROC =  0.6409574093192305\nareaUnderPR =  0.41581920691593866\nf1 =  0.7236853215821056\nweightedPrecision =  0.7596274970853623\nweightedRecall =  0.7888157830563886\naccuracy =  0.7888157830563886\n</div>"]}}],"execution_count":223},{"cell_type":"code","source":["metrics = loaded_prediction_and_labels.select(\"label\", \"prediction\").rdd.map(tuple)\nmetrics = MulticlassMetrics(metrics)\nprint(\"Confusion Matrix:\", metrics.confusionMatrix().toArray())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Confusion Matrix: [[2984934.  772522.]\n [  50193.   88073.]]\n</div>"]}}],"execution_count":224},{"cell_type":"code","source":["y_true = loaded_prediction_and_labels.select(\"label\").collect()\ny_pred = loaded_prediction_and_labels.select(\"prediction\").collect()\nprint(classification_report(y_true, y_pred))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n         0.0       0.79      0.98      0.88   3035127\n         1.0       0.64      0.10      0.18    860595\n\n    accuracy                           0.79   3895722\n   macro avg       0.72      0.54      0.53   3895722\nweighted avg       0.76      0.79      0.72   3895722\n\n</div>"]}}],"execution_count":225},{"cell_type":"markdown","source":["[Slide16-Madhukar]"],"metadata":{}},{"cell_type":"markdown","source":["##Model Results"],"metadata":{}},{"cell_type":"markdown","source":["- 5-fold **CrossValidation** did not show any significant improvement over **TrainValidationSplit** in 3:1 ratio. Summary of both are shown below.\n>- Training time of CV was signficantly longer, understandably\n- **weightedPrecision** and **F1** scores considered for model performance evaluation\n- **weightedPrecision improved from LR-->GBT-->Stacking**\n- **F1 improved from LR-->GBT** (stacking did not show further improvement)"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/cross-validation%20results%20on%202015.png?token=AM6SFIRHQYAS4SXEZGLGGMK7GV6S6' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/cross-validation%20results%20on%202015.png?token=AM6SFIRHQYAS4SXEZGLGGMK7GV6S6' >"]}}],"execution_count":229},{"cell_type":"code","source":["#CV results on 2019 (LR and GBT)\ndisplayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/2019%20v3.png?token=AM6SFIVX2WZLEQOPWLHB52C7GYNWW' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/2019%20v3.png?token=AM6SFIVX2WZLEQOPWLHB52C7GYNWW' >"]}}],"execution_count":230},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/TSV%20results%20on%202019.png?token=AM6SFITEXSOVHGMM5YUB4XK7GV6ES' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/TSV%20results%20on%202019.png?token=AM6SFITEXSOVHGMM5YUB4XK7GV6ES' >"]}}],"execution_count":231},{"cell_type":"markdown","source":["#### Confusion Matrix"],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/CM%203.png?token=AM6SFIRVVQUEC6BKRIO2ODK7GV6QG' >\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<img src ='https://raw.githubusercontent.com/UCB-w261/su20-project-su20-team7/master/images/CM%203.png?token=AM6SFIRVVQUEC6BKRIO2ODK7GV6QG' >"]}}],"execution_count":233},{"cell_type":"markdown","source":["[Slide17-Max/Rad]"],"metadata":{}},{"cell_type":"markdown","source":["###Conclusions\n\n**Join Performance and Scalability**\n\nThe biggest join is between airlines (flights) and weather. We prepared for this by assigning each weather report to a time bucket and making sure to consider only the latest report in each bucket if there were multiple. Pyspark window functions were great for this. Window functions are very expressive and flexible and quite difficult to understand and test. \n\nThe scalability of this join was really not a problem, even with the largest datasets. However, correctness was difficult. We benefited by putting some strict assertions in our code to make sure that we had not dropped any data or inadvertently duplicated it.\n\n**Cloud Computing**\n\nBesides providing the necessary computing resources for our experiments, the cloud makes it possible for the proposed solution to be easily scalable. In fact, if the amount of data increases (e.g., by extending the analysis to many years of flight and weather data), the cloud can provide the required resources with a high level of elasticity, reliability, and scalability.\n\n**Solution Approach**\n\nThe following are the overall **merits** of our approach:\n\n - Many models do not account for weather, but we have a chance to now include weather, which has the potential to improve accuracy of predictions\n - any models predict aggregate delays at major airports which are not specific enough for most stakeholders involved to take action. Delay predictions of individual flights may fill this gap.\n - The pre-vision horizon for most models is 24 hrs, using weather, this can be upto 4-5 days in advance\n - However we did note that most of the reasons for delays are stochastic phenomena which are difficult to predict timely and accurately.\n\nOur **future work** could include:\n\n - Data balancing using sophisticated techniques such as SMOTE\n - Multi-class classification based on delay duration group - we imagine that in addition to knowing that a flight would be delayed, it would perhaps be more helpful to know whether a flight would be delayed by 15-30 minutes vs 30-45min.\n - Considering multiple time windows for weather related features for greater precision\n - Implementing some sort of page rank for the airports to reflect its capacity to handle delays"],"metadata":{}},{"cell_type":"markdown","source":["### Application of Course Concepts \n\n**Scalability**: \nOne of the first issues we confronted in this project is the sheer size of the dataset: over 300 million records of weather data and gigabytes of flight data. And while we were able to conduct our initial EDA and algorithm exploration on a much smaller subset, we needed to ensure every aspect of our approach was scalable to the entire dataset. We relied on the use of PySpark data structures such as DataFrames as well as keeping all our transformations and other operations on the Spark cluster of machines. This allowed us to scale from or first attempts in this project on the smaller dataset up to the complete version. Only our approach to EDA required a subtle change as we no longer run EDA on the entire dataset but instead on a 1% sample.\n\n**One Hot Encoding / Vector Embeddings**: \nWe wanted to train our LG model with certain categorical features such as Origin and Destination airports, an important determinant of departure delays as we showed in our EDA and feature selection sections. We employed helper functions VectorAssembler and RFormula provided by the ML Feature library in order to convert these columns from their original string data types into a series of columns that are one hot encoded to represent their respective airports. This transformation helps binarize the feature vector and make the model fit more efficient.\n\n**Model Complexity / bias variance tradeoff / regularization**: \nIf our model is too simple, then we would run the risk of underfitting with high bias. On the other hand, if the model is too complex with many features, then the number of features after OHE etc would explode leading to overfitting with high variance. This is bias-variance tradeoff, and can be partially averted by using ensemble methods like Random Forests, Gradient Boosted Trees etc. For non-ensemble algorithms like logistic regression, L1 and L2 regularization can be utilized. Very high L1 regularization would drive the weights to zero, and thus help to indirectly do feature selection. In our case, we implemented ElasticNet so we could tune the extent of L1 and L2 regularization."],"metadata":{}},{"cell_type":"markdown","source":["### Research Citations\n- The Economic Cost of Airline Flight Delay, Everett B. Peterson, Kevin Neels, Nathan Barczi and Thea Graham,: Journal of Transport Economics and Policy , January 2013, Vol. 47, No. 1 (January 2013), pp. 107-121\n\n- J. J. Rebollo and H. Balakrishnan. Characterization and prediction of air trac delays.Transportation Research Part C: Emerging Technologies, 44(Supplement C):231–241,July 2014\n\n- Using Scalable Data Mining for Predicting Flight Delays, L. Belcastro, Domenico Talia, Fabrizio Marozzo, Paolo Trunfio, ACM Transactions on Intelligent Systems and Technology 8(1) · January 2016\n\n- A Review on Flight Delay Prediction, Alice Sternberg, Jorge Soares, Diego Carvalho, Eduardo Ogasawara CEFET/RJ, Rio de Janeiro, Brazil, November 6, 2017\n\n- Bad Weather and Flight Delays: The Impact of Sudden and Slow Onset Weather Events, Stefan Borsky, Christian Unterberger, Economics of Transportation 18(June):10-26 · March 2019\n\n- A Methodology for Predicting Aggregate Flight Departure Delays in Airports Based on Supervised Learning, Bojia Ye, Bo Liu, Yong Tian, Lili Wan, MDPI Basel, Switzerland, April 2020"],"metadata":{}}],"metadata":{"name":"W261_SU20_FINAL_PROJECT_TEAM7","notebookId":2542098129699087},"nbformat":4,"nbformat_minor":0}
